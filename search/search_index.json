{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Projet Pr\u00e9diction du trafic cyclable de la m\u00e9tropole de Montpellier","text":"<p>Ce projet est une application de Machine Learning con\u00e7ue pour pr\u00e9dire le trafic cyclable journalier sur les compteurs de la m\u00e9tropole de Montpellier.</p> <p>Il int\u00e8gre l'ensemble du cycle de vie de la donn\u00e9e : de la collecte (ETL) \u00e0 la restitution (Frontend), en passant par l'entra\u00eenement du mod\u00e8le et le monitoring des performances.</p>"},{"location":"#credits","title":"Cr\u00e9dits","text":"<p>Lo\u00efc</p> <p>Saleh</p> <p>Ga\u00ebtan</p>"},{"location":"architecture/","title":"Architecture projet","text":"<p>Le projet est divis\u00e9 en deux blocs principaux conteneuris\u00e9s via Docker :</p> <ol> <li> <p>Backend : Le moteur de l'application g\u00e9rant la collecte, le traitement des donn\u00e9es ainsi que sa mise \u00e0 disposition pour l'entra\u00eenement du mod\u00e8le (XGBoost).</p> </li> <li> <p>Frontend : L'interface utilisateur pour visualiser les donn\u00e9es et les pr\u00e9dictions.</p> </li> </ol>"},{"location":"architecture/#arborescence","title":"Arborescence","text":"<pre><code>backend/\n\u251c\u2500\u2500 api/                            # Couche d'exposition (API REST)\n\u2502   \u251c\u2500\u2500 api.py                      # Point d'entree de l'application FastAPI\n\u2502   \u251c\u2500\u2500 endpoints.py                # Definition des routes (GET /predict, etc.)\n\u2502   \u2514\u2500\u2500 server.py                   # Configuration du serveur Uvicorn/Gunicorn\n\u251c\u2500\u2500 core/                           # Configuration coeur\n\u2502   \u251c\u2500\u2500 dependencies.py             # Gestion des instances (Singleton BDD)\n\u2502   \u2514\u2500\u2500 training.py                 # Constantes liees a l'entrainement\n\u251c\u2500\u2500 data/                           # Stockage local (Volume Docker)\n\u2502   \u251c\u2500\u2500 raw/                        # Fichiers CSV bruts temporaires\n\u2502   \u251c\u2500\u2500 models/                     # Artefacts ML (fichiers .pkl)\n\u2502   \u2514\u2500\u2500 output/                     # Exports de donnees traitees\n\u251c\u2500\u2500 database/                       # Couche de stockage\n\u2502   \u251c\u2500\u2500 database.py                 # Modeles SQLAlchemy (Tables SQL)\n\u2502   \u251c\u2500\u2500 fetch_prediction.py         # Requetes specifiques de lecture\n\u2502   \u2514\u2500\u2500 service.py                  # CRUD complet (Create, Read, Update, Delete)\n\u251c\u2500\u2500 download/                       # Connecteurs aux APIs externes (Extract)\n\u2502   \u251c\u2500\u2500 abstract_loader.py          # Classe mere abstraite pour les loaders\n\u2502   \u251c\u2500\u2500 daily_weather_api.py        # Client API OpenMeteo (Previsions J0)\n\u2502   \u251c\u2500\u2500 ecocounters_ids.py          # Recuperation de la liste des compteurs\n\u2502   \u251c\u2500\u2500 geocoding_service.py        # Service de conversion Adresse &lt;-&gt; GPS\n\u2502   \u251c\u2500\u2500 trafic_history_api.py       # Client API EcoCompteur (Historique avec pagination)\n\u2502   \u2514\u2500\u2500 weeather_api.py             # Client API OpenMeteo (Archive historique)\n\u251c\u2500\u2500 features/                       # Ingenierie des fonctionnalites (Transform)\n\u2502   \u251c\u2500\u2500 features_engineering.py     # Transformation Donnee brute -&gt; Variables ML\n\u2502   \u2514\u2500\u2500 features_vizualization.py   # Outils graphiques pour analyser les features\n\u251c\u2500\u2500 modeling/                       # Coeur du Machine Learning\n\u2502   \u251c\u2500\u2500 predictor.py                # Moteur d'inference (Charge le modele et predit)\n\u2502   \u251c\u2500\u2500 preprocessor.py             # Transformation (Scaling, Encodage)\n\u2502   \u2514\u2500\u2500 trainer.py                  # Entrainement (GridSearch, CrossVal, Save)\n\u251c\u2500\u2500 monitoring/                     # Suivi de la qualite\n\u2502   \u2514\u2500\u2500 performance.py              # Calcul des metriques (MAE, RMSE) reel vs predit\n\u251c\u2500\u2500 pipelines/                      # Scripts d'orchestration (Workflows)\n\u2502   \u251c\u2500\u2500 daily_predictor.py          # Pipeline journalier : Prediction J0\n\u2502   \u251c\u2500\u2500 daily_update.py             # Pipeline journalier : Mise a jour J-1 + Monitoring\n\u2502   \u251c\u2500\u2500 data_insertion.py           # Logique d'insertion securisee en BDD\n\u2502   \u251c\u2500\u2500 initialize_project.py       # Script d'installation initiale (Historique complet)\n\u2502   \u251c\u2500\u2500 model_training.py           # Pipeline de re-entrainement complet\n\u2502   \u251c\u2500\u2500 pipeline.py                 # Menu interactif (CLI)\n\u2502   \u2514\u2500\u2500 pipeline_visualization.py   # Generation de graphes pour le pipeline\n\u251c\u2500\u2500 src/                            # Utilitaires de traitement de donnees\n\u2502   \u251c\u2500\u2500 api_data_processing.py      # Nettoyage specifique aux retours API\n\u2502   \u251c\u2500\u2500 data_cleaner.py             # Fonctions de nettoyage (dedoublonnage, types)\n\u2502   \u251c\u2500\u2500 data_exploration.py         # Classe pour generer des stats descriptives\n\u2502   \u2514\u2500\u2500 data_merger.py              # Logique de fusion (Merge Trafic + Meteo)\n\u251c\u2500\u2500 tests/                          # Tests unitaires et d'integration\n\u2502   \u251c\u2500\u2500 conftest.py                 # Configuration Pytest (Fixtures)\n\u2502   \u251c\u2500\u2500 test_api.py                 # Tests des endpoints API\n\u2502   \u251c\u2500\u2500 test_data_insertion.py      # Tests des ecritures en BDD\n\u2502   \u251c\u2500\u2500 test_database_service.py    # Tests des requetes SQL\n\u2502   \u2514\u2500\u2500 test_train.py               # Tests du pipeline d'entrainement\n\u251c\u2500\u2500 utils/                          # Outils transverses\n\u2502   \u251c\u2500\u2500 logging_config.py           # Configuration centralisee des logs\n\u2502   \u251c\u2500\u2500 paths.py                    # Gestion des chemins absolus\n\u2502   \u2514\u2500\u2500 weather_utils.py            # Parsing des reponses OpenMeteo\n\u251c\u2500\u2500 .dockerignore                   # Fichiers exclus du build Docker\n\u251c\u2500\u2500 Dockerfile                      # Definition de l'image Backend\n\u251c\u2500\u2500 main.py                         # Point d'entree CLI (Menu principal)\n\u251c\u2500\u2500 main_initialize.py              # Raccourci pour l'initialisation\n\u2514\u2500\u2500 requirements.txt                # Dependances Python\nfrontend/\n\u251c\u2500\u2500 app.py                          # Point d'entree de l'application Web\n\u251c\u2500\u2500 components.py                   # Widgets graphiques reutilisables\n\u251c\u2500\u2500 data.py                         # Connecteur pour recuperer les donnees du Backend\n\u251c\u2500\u2500 plots.py                        # Fonctions de generation de graphiques\n\u251c\u2500\u2500 requirements.txt                # Dependances Frontend\n\u251c\u2500\u2500 .dockerignore\n\u2514\u2500\u2500 Dockerfile                      # Definition de l'image Frontend\n.gitignore                          # Fichiers exclus de Git\nREADME.md                           # Documentation du projet\ndocker-compose.yml                  # Orchestration des conteneurs\nlogique_predict_daily.md            # Doc specifique sur la logique J0\nprometheus.yml                      # Configuration monitoring infrastructure\nreset_db.py                         # Script utilitaire pour vider la base\n</code></pre>"},{"location":"architecture/#modelisation-c4","title":"Mod\u00e9lisation (C4)","text":""},{"location":"architecture/#diagramme-de-contexte","title":"Diagramme de Contexte","text":"<pre><code>C4Context\n    title Diagramme de Contexte - Syst\u00e8me de Pr\u00e9vision V\u00e9lo\n\n    Person(user, \"Utilisateur Final\", \"Urbaniste ou Citoyen de Montpellier\")\n\n    System(system, \"Application Pr\u00e9vision V\u00e9lo\", \"Permet de visualiser l'historique et les pr\u00e9dictions de trafic cyclable.\")\n\n    System_Ext(api_mmm, \"API Open Data Montpellier\", \"Fournit les comptages r\u00e9els (J-1)\")\n    System_Ext(api_meteo, \"API OpenMeteo\", \"Fournit l'historique et les pr\u00e9visions m\u00e9t\u00e9o (J0)\")\n\n    Rel(user, system, \"Consulte les dashboards\", \"HTTPS\")\n    Rel(system, api_mmm, \"R\u00e9cup\u00e8re les donn\u00e9es trafic\", \"JSON/HTTPS\")\n    Rel(system, api_meteo, \"R\u00e9cup\u00e8re la m\u00e9t\u00e9o\", \"JSON/HTTPS\")</code></pre>"},{"location":"architecture/#diagramme-de-conteneur","title":"Diagramme de Conteneur","text":"<pre><code>C4Container\n    title Diagramme de Conteneurs - Architecture Docker/Azure\n\n    Person(user, \"Utilisateur\", \"Navigateur Web\")\n\n    Container_Boundary(azure, \"Azure Cloud\") {\n\n        Container(frontend, \"Frontend\", \"Python, NiceGUI\", \"Interface utilisateur interactive pour la visualisation.\")\n\n        Container(backend, \"Backend API &amp; Worker\", \"Python, FastAPI\", \"G\u00e8re l'ETL, l'entra\u00eenement du mod\u00e8le et l'exposition des donn\u00e9es.\")\n\n        ContainerDb(database, \"Base de Donn\u00e9es\", \"Azure SQL Database\", \"Stocke l'historique, les m\u00e9tadonn\u00e9es et les pr\u00e9dictions.\")\n\n        Container(storage, \"Stockage Local / Volume\", \"Syst\u00e8me de Fichiers\", \"Stocke les mod\u00e8les entra\u00een\u00e9s (.pkl) et les CSV temporaires.\")\n    }\n\n    System_Ext(apis, \"APIs Externes\", \"Montpellier &amp; M\u00e9t\u00e9o\")\n\n    Rel(user, frontend, \"Visite\", \"HTTPS\")\n    Rel(frontend, backend, \"Requ\u00eate les donn\u00e9es\", \"HTTP/JSON\")\n\n    Rel(backend, database, \"Lit/Ecrit les donn\u00e9es\", \"SQL/SQLAlchemy\")\n    Rel(backend, storage, \"Charge/Sauvegarde les mod\u00e8les\", \"File System\")\n    Rel(backend, apis, \"T\u00e9l\u00e9charge les donn\u00e9es\", \"HTTPS\")</code></pre>"},{"location":"architecture/#diagramme-de-composant","title":"Diagramme de Composant","text":"<pre><code>C4Component\n    title Diagramme de Composants - Backend Python\n\n    Container_Boundary(backend, \"Backend Application\") {\n\n        Component(orchestrator, \"Pipelines Orchestrator\", \"daily_prediction.py, daily_update.py\", \"Coordonne les t\u00e2ches planifi\u00e9es (CRON).\")\n\n        Component(loader, \"Data Loaders\", \"download/*\", \"G\u00e8re la connexion et la pagination avec les APIs externes.\")\n\n        Component(feature_eng, \"Feature Engineer\", \"features_engineering.py\", \"Transforme les donn\u00e9es brutes en vecteurs (Lags, Sinus, M\u00e9t\u00e9o).\")\n\n        Component(predictor, \"ML Predictor\", \"modeling/predictor.py\", \"Charge le mod\u00e8le XGBoost et effectue l'inf\u00e9rence.\")\n\n        Component(trainer, \"ML Trainer\", \"modeling/trainer.py\", \"Entra\u00eene et sauvegarde le mod\u00e8le (Optionnel en prod).\")\n\n        Component(db_service, \"Database Service\", \"database/service.py\", \"Abstraction CRUD pour parler \u00e0 la BDD.\")\n\n        Component(api_routes, \"API Endpoints\", \"api/endpoints.py\", \"Expose les r\u00e9sultats au Frontend.\")\n    }\n\n    ContainerDb(db, \"Database\", \"SQL Server\")\n    System_Ext(ext_api, \"External APIs\")\n\n    Rel(orchestrator, loader, \"D\u00e9clenche la collecte\")\n    Rel(loader, ext_api, \"Requ\u00eate\")\n\n    Rel(orchestrator, db_service, \"Sauvegarde/Lit l'historique\")\n    Rel(db_service, db, \"SQL Query\")\n\n    Rel(orchestrator, feature_eng, \"Transforme les donn\u00e9es\")\n    Rel(orchestrator, predictor, \"Demande une pr\u00e9diction\")\n\n    Rel(predictor, feature_eng, \"Utilise pour le preprocessing\")\n    Rel(api_routes, db_service, \"Lit les r\u00e9sultats finaux\")</code></pre>"},{"location":"interface/frontend_overview/","title":"Documentation de l'Interface Utilisateur (Frontend)","text":"<p>Cette page d\u00e9crit l'architecture, les fonctionnalit\u00e9s et la logique de l'interface utilisateur (frontend) du projet, d\u00e9velopp\u00e9e avec le framework NiceGUI.</p>"},{"location":"interface/frontend_overview/#1-vue-densemble","title":"1. Vue d'ensemble","text":"<p>L'interface utilisateur a pour but de fournir une visualisation interactive et en temps r\u00e9el des donn\u00e9es de comptage de v\u00e9los \u00e0 Montpellier. Elle permet aux utilisateurs de : - Consulter les pr\u00e9dictions de trafic pour le jour m\u00eame. - Analyser les performances des pr\u00e9dictions pass\u00e9es. - Explorer les tendances historiques et les profils d'utilisation de chaque compteur.</p>"},{"location":"interface/frontend_overview/#2-architecture-et-structure-des-fichiers","title":"2. Architecture et Structure des Fichiers","text":"<p>Le frontend est organis\u00e9 en modules clairs, chacun ayant une responsabilit\u00e9 unique, ce qui facilite la maintenance et l'\u00e9volution de l'application.</p> <ul> <li> <p><code>app.py</code>: Point d'entr\u00e9e principal. Il initialise l'application, d\u00e9finit la structure de la page (header, layout) et g\u00e8re l'\u00e9tat global comme la s\u00e9lection du compteur.</p> </li> <li> <p><code>components.py</code>: C\u0153ur de l'affichage dynamique. Contient la logique pour afficher les KPIs, les cartes et les graphiques pour un compteur donn\u00e9. Ce composant est \"rafra\u00eechissable\" pour des mises \u00e0 jour fluides.</p> </li> <li> <p><code>data.py</code>: Couche d'acc\u00e8s aux donn\u00e9es. Centralise toute la communication avec les APIs externes (le backend du projet et l'API m\u00e9t\u00e9o). Il est con\u00e7u pour \u00eatre robuste et configurable.</p> </li> <li> <p><code>plots.py</code>: Module de visualisation. Regroupe toutes les fonctions qui g\u00e9n\u00e8rent les graphiques avec Matplotlib, assurant une s\u00e9paration nette entre la logique de donn\u00e9es et la pr\u00e9sentation visuelle.</p> </li> </ul>"},{"location":"interface/frontend_overview/#3-fonctionnalites-detaillees","title":"3. Fonctionnalit\u00e9s D\u00e9taill\u00e9es","text":""},{"location":"interface/frontend_overview/#layout-principal-et-gestion-de-letat-apppy","title":"Layout Principal et Gestion de l'\u00c9tat (<code>app.py</code>)","text":"<p>L'application est construite autour d'une page unique avec une mise en page r\u00e9active.</p> <ul> <li>Header Persistant : Un en-t\u00eate contient le titre de l'application et un s\u00e9lecteur (<code>ui.select</code>) permettant de choisir une station de comptage.</li> <li>Gestion de l'\u00c9tat Client : L'application utilise <code>app.storage.client</code> pour m\u00e9moriser la station s\u00e9lectionn\u00e9e par l'utilisateur. Cela garantit que si l'utilisateur rafra\u00eechit la page, sa derni\u00e8re s\u00e9lection est conserv\u00e9e, am\u00e9liorant l'exp\u00e9rience utilisateur.</li> <li>Rafra\u00eechissement Dynamique : Lorsque l'utilisateur change de station, l'\u00e9v\u00e9nement <code>on_change</code> du s\u00e9lecteur appelle la m\u00e9thode <code>.refresh()</code> du composant <code>render_counter_content</code>. Cela met \u00e0 jour uniquement la partie centrale de la page, sans rechargement complet.</li> </ul>"},{"location":"interface/frontend_overview/#affichage-du-contenu-componentspy","title":"Affichage du Contenu (<code>components.py</code>)","text":"<p>La fonction <code>@ui.refreshable render_counter_content</code> est le moteur de l'interface.</p> <ul> <li> <p>Gestion des Donn\u00e9es Obsol\u00e8tes ou Manquantes : Une des fonctionnalit\u00e9s cl\u00e9s est la capacit\u00e9 \u00e0 informer l'utilisateur de l'\u00e9tat des donn\u00e9es.</p> <ul> <li>Si la pr\u00e9diction affich\u00e9e ne date pas du jour m\u00eame, un bandeau d'avertissement orange est affich\u00e9.</li> <li>Si aucune pr\u00e9diction n'est disponible pour un compteur, un bandeau d'erreur rouge l'indique clairement. Cela rend l'application transparente sur la fra\u00eecheur des informations qu'elle pr\u00e9sente.</li> </ul> </li> <li> <p>Navigation par Onglets : Le contenu est organis\u00e9 en deux onglets pour une meilleure clart\u00e9 :</p> <ol> <li>Tableau de Bord : Affiche les informations essentielles en un coup d'\u0153il (KPIs, carte, m\u00e9t\u00e9o).</li> <li>Analyses &amp; Stats : Regroupe les graphiques d'analyse historique.</li> </ol> </li> </ul>"},{"location":"interface/frontend_overview/#couche-de-donnees-robuste-datapy","title":"Couche de Donn\u00e9es Robuste (<code>data.py</code>)","text":"<p>Ce module est con\u00e7u pour \u00eatre \u00e0 la fois flexible et r\u00e9silient.</p> <ul> <li> <p>Configuration Intelligente de l'URL de l'API : La fonction <code>_get_api_url</code> d\u00e9tecte automatiquement l'environnement d'ex\u00e9cution (variable d'environnement pour la production, pr\u00e9sence de <code>/.dockerenv</code> pour Docker, ou <code>localhost</code> pour le d\u00e9veloppement local). Cela permet de d\u00e9ployer l'application dans diff\u00e9rents contextes sans aucune modification du code.</p> </li> <li> <p>Mise en Cache Efficace : La liste des compteurs est r\u00e9cup\u00e9r\u00e9e une seule fois et stock\u00e9e dans <code>_COUNTERS_CACHE</code>. Cela \u00e9vite des appels r\u00e9seau inutiles et acc\u00e9l\u00e8re les chargements ult\u00e9rieurs de la page.</p> </li> <li> <p>Gestion des Erreurs d'API : La fonction <code>get_dashboard_data</code> est encapsul\u00e9e dans un bloc <code>try...except</code>. En cas d'\u00e9chec de la connexion \u00e0 l'API (timeout, erreur 500, etc.), elle ne fait pas planter l'application. Au lieu de cela, elle retourne une structure de donn\u00e9es \"vide\" mais valide. Cela permet \u00e0 l'interface de rester fonctionnelle et d'afficher un \u00e9tat d\u00e9grad\u00e9 propre au lieu d'une page d'erreur.</p> </li> </ul>"},{"location":"interface/frontend_overview/#visualisations-claires-plotspy","title":"Visualisations Claires (<code>plots.py</code>)","text":"<p>Ce module isole compl\u00e8tement la logique de cr\u00e9ation des graphiques.</p> <ul> <li>Modularit\u00e9 : Chaque graphique (tendance 30 jours, fiabilit\u00e9 7 jours, etc.) est g\u00e9n\u00e9r\u00e9 par sa propre fonction.</li> <li>Style Coh\u00e9rent : Une fonction <code>apply_dashboard_style</code> est utilis\u00e9e pour appliquer un style visuel commun \u00e0 tous les graphiques, garantissant une apparence professionnelle et homog\u00e8ne.</li> <li>Gestion des Donn\u00e9es Vides : Chaque fonction de plot v\u00e9rifie si les donn\u00e9es d'entr\u00e9e sont valides et fournit des valeurs par d\u00e9faut si n\u00e9cessaire, ce qui contribue \u00e0 la robustesse g\u00e9n\u00e9rale de l'affichage.</li> </ul>"},{"location":"interface/frontend_overview/#4-lancement-de-lapplication","title":"4. Lancement de l'Application","text":"<p>L'application est lanc\u00e9e via la commande <code>ui.run()</code> \u00e0 la fin du fichier <code>app.py</code>.</p> <pre><code>ui.run(host=\"0.0.0.0\", port=8080, title=\"V\u00e9lo Montpellier IA\", favicon=\"\ud83d\udeb4\")\n</code></pre> <ul> <li><code>host=\"0.0.0.0\"</code> rend l'application accessible sur le r\u00e9seau (essentiel pour Docker).</li> <li><code>port=8080</code> est le port d'\u00e9coute par d\u00e9faut.</li> </ul>"},{"location":"ml/features/","title":"Ing\u00e9nierie des Fonctionnalit\u00e9s","text":"<p>La transformation des donn\u00e9es brutes en vecteurs math\u00e9matiques est g\u00e9r\u00e9e par cette classe centrale.</p> <p>Class responsible for creating, transforming and enriching features for machine learning model training.</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>class FeaturesEngineering:\n    \"\"\"\n    Class responsible for creating, transforming and enriching features\n    for machine learning model training.\n    \"\"\"\n\n    def __init__(self, df: pd.DataFrame):\n        \"\"\"\n        Initialize the class with a copy of the dataframe.\n\n        Args:\n            df (pd.DataFrame): Input dataset containing at least a 'date' column.\n        \"\"\"\n        self.df: pd.DataFrame = df.copy()\n        print(\n            \"[INFO] FeaturesEngineering initialized with dataframe of shape:\",\n            self.df.shape,\n        )\n\n        # -----------------------------------------------------------\n    def remove_suspect_counters(self, suspects: list = None) -&gt; \"FeaturesEngineering\":\n        \"\"\"\n        Removes rows corresponding to a list of suspect station IDs.\n\n        Args:\n            suspects (list): List of station_id strings to remove. \n                             If None, uses a default hardcoded list.\n\n        Returns:\n            self (FeaturesEngineering): method chaining\n        \"\"\"\n        print(\"[STEP] Removing suspect counters...\")\n\n        # Liste par d\u00e9faut fournie dans ta demande\n        if suspects is None:\n            suspects = [\n                \"urn:ngsi-ld:EcoCounter:867228050089043\",\n                \"urn:ngsi-ld:EcoCounter:867228050089159\",\n                \"urn:ngsi-ld:EcoCounter:867228050089217\",\n                \"urn:ngsi-ld:EcoCounter:867228050089787\",\n                \"urn:ngsi-ld:EcoCounter:867228050092989\"\n            ]\n\n        # On compte avant pour le log\n        initial_count = len(self.df)\n\n        # Le tilde (~) signifie \"NOT\" en Pandas\n        # On garde tout ce qui N'EST PAS dans la liste des suspects\n        self.df = self.df[~self.df['station_id'].isin(suspects)]\n\n        removed_count = initial_count - len(self.df)\n\n        print(f\"[INFO] Removed {removed_count} rows from suspect counters.\")\n        print(f\"[INFO] New dataframe shape: {self.df.shape}\")\n\n        return self\n\n    # -----------------------------------------------------------\n    def add_week_month_year(self) -&gt; \"FeaturesEngineering\":\n        \"\"\"\n        Extract date-related features: day_of_week, month, year,\n        day_of_year, and is_weekend.\n\n        Returns:\n            self (FeaturesEngineering): method chaining\n        \"\"\"\n        print(\"[STEP] Adding week, month, year features...\")\n\n        self.df[\"date\"] = pd.to_datetime(self.df[\"date\"])\n\n        self.df[\"day_of_week\"] = self.df[\"date\"].dt.dayofweek\n        self.df[\"month\"] = self.df[\"date\"].dt.month\n        self.df[\"year\"] = self.df[\"date\"].dt.year\n        self.df[\"day_of_year\"] = self.df[\"date\"].dt.dayofyear\n        self.df[\"is_weekend\"] = self.df[\"day_of_week\"].isin([5, 6]).astype(int)\n\n        print(\"[INFO] Added date decomposition features. Sample:\")\n        print(self.df.head(2))\n        return self\n\n    # -----------------------------------------------------------\n    def Cycliques(self) -&gt; \"FeaturesEngineering\":\n        \"\"\"\n        Add cyclical encoding for day_of_week and month.\n\n        Returns:\n            self (FeaturesEngineering): method chaining\n        \"\"\"\n        print(\"[STEP] Adding cyclical features...\")\n\n        self.df[\"day_of_week_sin\"] = np.sin(2 * np.pi * self.df[\"day_of_week\"] / 7)\n        self.df[\"day_of_week_cos\"] = np.cos(2 * np.pi * self.df[\"day_of_week\"] / 7)\n\n        self.df[\"month_sin\"] = np.sin(2 * np.pi * (self.df[\"month\"] - 1) / 12)\n        self.df[\"month_cos\"] = np.cos(2 * np.pi * (self.df[\"month\"] - 1) / 12)\n\n        print(\"[INFO] Added cyclical features. Sample:\")\n        print(self.df.head(2))\n        return self\n\n    # -----------------------------------------------------------\n    def add_weather_featuers(self) -&gt; \"FeaturesEngineering\":\n        \"\"\"\n        Create weather-based binary features from precipitation,\n        temperature and wind.\n\n        Returns:\n            self (FeaturesEngineering): method chaining\n        \"\"\"\n        print(\"[STEP] Adding weather features...\")\n\n        self.df[\"is_rainy\"] = (self.df[\"precipitation_mm\"] &gt; 1.0).astype(int)\n        self.df[\"is_cold\"] = (self.df[\"avg_temp\"] &lt; 5.0).astype(int)\n        self.df[\"is_hot\"] = (self.df[\"avg_temp\"] &gt; 30.0).astype(int)\n        self.df[\"is_windy\"] = (self.df[\"vent_max\"] &gt; 30.0).astype(int)\n\n        print(\"[INFO] Added weather features. Sample:\")\n        print(self.df.head(2))\n        return self\n\n    # -----------------------------------------------------------\n    def lag(self) -&gt; \"FeaturesEngineering\":\n        \"\"\"\n        Add lag features (lag_1 and lag_7) grouped by station_id.\n\n        Returns:\n            self (FeaturesEngineering): method chaining\n        \"\"\"\n        print(\"[STEP] Adding lag features...\")\n\n        self.df = self.df.sort_values(by=[\"station_id\", \"date\"])\n\n        self.df[\"lag_1\"] = self.df.groupby(\"station_id\")[\"intensity\"].shift(1)\n        self.df[\"lag_7\"] = self.df.groupby(\"station_id\")[\"intensity\"].shift(7)\n\n        # drop rows with missing lags\n        self.df = self.df.dropna(subset=[\"lag_1\", \"lag_7\"])\n\n        print(\"[INFO] Lag features added.\")\n        return self\n\n    # -----------------------------------------------------------\n    def add_holidays_feature(self) -&gt; \"FeaturesEngineering\":\n        \"\"\"\n        Add a boolean 'is_holiday' feature using French official holidays.\n\n        Returns:\n            self (FeaturesEngineering): method chaining\n        \"\"\"\n        print(\"[STEP] Adding holiday feature...\")\n\n        self.df[\"date\"] = pd.to_datetime(self.df[\"date\"])\n        years = self.df[\"date\"].dt.year.unique()\n        year_range = range(min(years), max(years) + 2)\n\n        fr_holidays = holidays.FR(years=year_range)\n        self.holidays = fr_holidays\n\n        self.df[\"is_holiday\"] = self.df[\"date\"].dt.date.apply(\n            lambda d: d in fr_holidays\n        )\n\n        print(\"[INFO] Holiday feature added. Sample:\")\n        print(self.df.head(2))\n        return self\n\n    # -----------------------------------------------------------\n    def drop_date_column(self) -&gt; \"FeaturesEngineering\":\n        \"\"\"\n        Drop the original 'date' column from the dataframe.\n\n        Returns:\n            self (FeaturesEngineering): method chaining\n        \"\"\"\n        print(\"[STEP] Dropping 'date' column...\")\n\n        if \"date\" in self.df.columns:\n            self.df = self.df.drop(columns=[\"date\"])\n            print(\"[INFO] 'date' column dropped.\")\n        else:\n            print(\"[WARNING] 'date' column was already missing.\")\n\n        return self\n\n    # -----------------------------------------------------------\n    def save_to_csv(\n        self, path: str = OUTPUT_PATH, filename: str = \"features_eng_data.csv\"\n    ) -&gt; None:\n        \"\"\"\n        Save final dataframe to CSV.\n\n        Args:\n            path (str | Path): directory where file will be saved\n            filename (str): name of the output file\n        \"\"\"\n        print(f\"[STEP] Saving dataframe to CSV: {filename}\")\n        file_path = path / filename\n        self.df.to_csv(file_path, index=False)\n        print(f\"[INFO] File saved successfully at: {file_path}\")\n\n    # -----------------------------------------------------------\n    def get_data(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Return the final processed dataframe.\n\n        Returns:\n            pd.DataFrame: final dataset\n        \"\"\"\n        print(\"[INFO] Returning final processed dataframe. Shape:\", self.df.shape)\n        return self.df\n</code></pre> <p>handler: python options: members: - add_week_month_year - Cycliques - lag - remove_suspect_counters</p>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.Cycliques","title":"<code>Cycliques()</code>","text":"<p>Add cyclical encoding for day_of_week and month.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>FeaturesEngineering</code> <p>method chaining</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def Cycliques(self) -&gt; \"FeaturesEngineering\":\n    \"\"\"\n    Add cyclical encoding for day_of_week and month.\n\n    Returns:\n        self (FeaturesEngineering): method chaining\n    \"\"\"\n    print(\"[STEP] Adding cyclical features...\")\n\n    self.df[\"day_of_week_sin\"] = np.sin(2 * np.pi * self.df[\"day_of_week\"] / 7)\n    self.df[\"day_of_week_cos\"] = np.cos(2 * np.pi * self.df[\"day_of_week\"] / 7)\n\n    self.df[\"month_sin\"] = np.sin(2 * np.pi * (self.df[\"month\"] - 1) / 12)\n    self.df[\"month_cos\"] = np.cos(2 * np.pi * (self.df[\"month\"] - 1) / 12)\n\n    print(\"[INFO] Added cyclical features. Sample:\")\n    print(self.df.head(2))\n    return self\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.__init__","title":"<code>__init__(df)</code>","text":"<p>Initialize the class with a copy of the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataset containing at least a 'date' column.</p> required Source code in <code>backend/features/features_engineering.py</code> <pre><code>def __init__(self, df: pd.DataFrame):\n    \"\"\"\n    Initialize the class with a copy of the dataframe.\n\n    Args:\n        df (pd.DataFrame): Input dataset containing at least a 'date' column.\n    \"\"\"\n    self.df: pd.DataFrame = df.copy()\n    print(\n        \"[INFO] FeaturesEngineering initialized with dataframe of shape:\",\n        self.df.shape,\n    )\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.add_holidays_feature","title":"<code>add_holidays_feature()</code>","text":"<p>Add a boolean 'is_holiday' feature using French official holidays.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>FeaturesEngineering</code> <p>method chaining</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def add_holidays_feature(self) -&gt; \"FeaturesEngineering\":\n    \"\"\"\n    Add a boolean 'is_holiday' feature using French official holidays.\n\n    Returns:\n        self (FeaturesEngineering): method chaining\n    \"\"\"\n    print(\"[STEP] Adding holiday feature...\")\n\n    self.df[\"date\"] = pd.to_datetime(self.df[\"date\"])\n    years = self.df[\"date\"].dt.year.unique()\n    year_range = range(min(years), max(years) + 2)\n\n    fr_holidays = holidays.FR(years=year_range)\n    self.holidays = fr_holidays\n\n    self.df[\"is_holiday\"] = self.df[\"date\"].dt.date.apply(\n        lambda d: d in fr_holidays\n    )\n\n    print(\"[INFO] Holiday feature added. Sample:\")\n    print(self.df.head(2))\n    return self\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.add_weather_featuers","title":"<code>add_weather_featuers()</code>","text":"<p>Create weather-based binary features from precipitation, temperature and wind.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>FeaturesEngineering</code> <p>method chaining</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def add_weather_featuers(self) -&gt; \"FeaturesEngineering\":\n    \"\"\"\n    Create weather-based binary features from precipitation,\n    temperature and wind.\n\n    Returns:\n        self (FeaturesEngineering): method chaining\n    \"\"\"\n    print(\"[STEP] Adding weather features...\")\n\n    self.df[\"is_rainy\"] = (self.df[\"precipitation_mm\"] &gt; 1.0).astype(int)\n    self.df[\"is_cold\"] = (self.df[\"avg_temp\"] &lt; 5.0).astype(int)\n    self.df[\"is_hot\"] = (self.df[\"avg_temp\"] &gt; 30.0).astype(int)\n    self.df[\"is_windy\"] = (self.df[\"vent_max\"] &gt; 30.0).astype(int)\n\n    print(\"[INFO] Added weather features. Sample:\")\n    print(self.df.head(2))\n    return self\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.add_week_month_year","title":"<code>add_week_month_year()</code>","text":"<p>Extract date-related features: day_of_week, month, year, day_of_year, and is_weekend.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>FeaturesEngineering</code> <p>method chaining</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def add_week_month_year(self) -&gt; \"FeaturesEngineering\":\n    \"\"\"\n    Extract date-related features: day_of_week, month, year,\n    day_of_year, and is_weekend.\n\n    Returns:\n        self (FeaturesEngineering): method chaining\n    \"\"\"\n    print(\"[STEP] Adding week, month, year features...\")\n\n    self.df[\"date\"] = pd.to_datetime(self.df[\"date\"])\n\n    self.df[\"day_of_week\"] = self.df[\"date\"].dt.dayofweek\n    self.df[\"month\"] = self.df[\"date\"].dt.month\n    self.df[\"year\"] = self.df[\"date\"].dt.year\n    self.df[\"day_of_year\"] = self.df[\"date\"].dt.dayofyear\n    self.df[\"is_weekend\"] = self.df[\"day_of_week\"].isin([5, 6]).astype(int)\n\n    print(\"[INFO] Added date decomposition features. Sample:\")\n    print(self.df.head(2))\n    return self\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.drop_date_column","title":"<code>drop_date_column()</code>","text":"<p>Drop the original 'date' column from the dataframe.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>FeaturesEngineering</code> <p>method chaining</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def drop_date_column(self) -&gt; \"FeaturesEngineering\":\n    \"\"\"\n    Drop the original 'date' column from the dataframe.\n\n    Returns:\n        self (FeaturesEngineering): method chaining\n    \"\"\"\n    print(\"[STEP] Dropping 'date' column...\")\n\n    if \"date\" in self.df.columns:\n        self.df = self.df.drop(columns=[\"date\"])\n        print(\"[INFO] 'date' column dropped.\")\n    else:\n        print(\"[WARNING] 'date' column was already missing.\")\n\n    return self\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.get_data","title":"<code>get_data()</code>","text":"<p>Return the final processed dataframe.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: final dataset</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def get_data(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Return the final processed dataframe.\n\n    Returns:\n        pd.DataFrame: final dataset\n    \"\"\"\n    print(\"[INFO] Returning final processed dataframe. Shape:\", self.df.shape)\n    return self.df\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.lag","title":"<code>lag()</code>","text":"<p>Add lag features (lag_1 and lag_7) grouped by station_id.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>FeaturesEngineering</code> <p>method chaining</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def lag(self) -&gt; \"FeaturesEngineering\":\n    \"\"\"\n    Add lag features (lag_1 and lag_7) grouped by station_id.\n\n    Returns:\n        self (FeaturesEngineering): method chaining\n    \"\"\"\n    print(\"[STEP] Adding lag features...\")\n\n    self.df = self.df.sort_values(by=[\"station_id\", \"date\"])\n\n    self.df[\"lag_1\"] = self.df.groupby(\"station_id\")[\"intensity\"].shift(1)\n    self.df[\"lag_7\"] = self.df.groupby(\"station_id\")[\"intensity\"].shift(7)\n\n    # drop rows with missing lags\n    self.df = self.df.dropna(subset=[\"lag_1\", \"lag_7\"])\n\n    print(\"[INFO] Lag features added.\")\n    return self\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.remove_suspect_counters","title":"<code>remove_suspect_counters(suspects=None)</code>","text":"<p>Removes rows corresponding to a list of suspect station IDs.</p> <p>Parameters:</p> Name Type Description Default <code>suspects</code> <code>list</code> <p>List of station_id strings to remove.               If None, uses a default hardcoded list.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>FeaturesEngineering</code> <p>method chaining</p> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def remove_suspect_counters(self, suspects: list = None) -&gt; \"FeaturesEngineering\":\n    \"\"\"\n    Removes rows corresponding to a list of suspect station IDs.\n\n    Args:\n        suspects (list): List of station_id strings to remove. \n                         If None, uses a default hardcoded list.\n\n    Returns:\n        self (FeaturesEngineering): method chaining\n    \"\"\"\n    print(\"[STEP] Removing suspect counters...\")\n\n    # Liste par d\u00e9faut fournie dans ta demande\n    if suspects is None:\n        suspects = [\n            \"urn:ngsi-ld:EcoCounter:867228050089043\",\n            \"urn:ngsi-ld:EcoCounter:867228050089159\",\n            \"urn:ngsi-ld:EcoCounter:867228050089217\",\n            \"urn:ngsi-ld:EcoCounter:867228050089787\",\n            \"urn:ngsi-ld:EcoCounter:867228050092989\"\n        ]\n\n    # On compte avant pour le log\n    initial_count = len(self.df)\n\n    # Le tilde (~) signifie \"NOT\" en Pandas\n    # On garde tout ce qui N'EST PAS dans la liste des suspects\n    self.df = self.df[~self.df['station_id'].isin(suspects)]\n\n    removed_count = initial_count - len(self.df)\n\n    print(f\"[INFO] Removed {removed_count} rows from suspect counters.\")\n    print(f\"[INFO] New dataframe shape: {self.df.shape}\")\n\n    return self\n</code></pre>"},{"location":"ml/features/#features.features_engineering.FeaturesEngineering.save_to_csv","title":"<code>save_to_csv(path=OUTPUT_PATH, filename='features_eng_data.csv')</code>","text":"<p>Save final dataframe to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>directory where file will be saved</p> <code>OUTPUT_PATH</code> <code>filename</code> <code>str</code> <p>name of the output file</p> <code>'features_eng_data.csv'</code> Source code in <code>backend/features/features_engineering.py</code> <pre><code>def save_to_csv(\n    self, path: str = OUTPUT_PATH, filename: str = \"features_eng_data.csv\"\n) -&gt; None:\n    \"\"\"\n    Save final dataframe to CSV.\n\n    Args:\n        path (str | Path): directory where file will be saved\n        filename (str): name of the output file\n    \"\"\"\n    print(f\"[STEP] Saving dataframe to CSV: {filename}\")\n    file_path = path / filename\n    self.df.to_csv(file_path, index=False)\n    print(f\"[INFO] File saved successfully at: {file_path}\")\n</code></pre>"},{"location":"ml/modeling/","title":"Mod\u00e9lisation IA","text":"<p>Cette section d\u00e9taille les classes responsables de l'entra\u00eenement et de l'inf\u00e9rence.</p>"},{"location":"ml/modeling/#le-predicteur-inference","title":"Le Pr\u00e9dicteur (Inf\u00e9rence)","text":"<p>C'est la classe utilis\u00e9e par le script journalier pour pr\u00e9dire J0.</p> Source code in <code>backend/modeling/predictor.py</code> <pre><code>class TrafficPredictor:\n    def __init__(\n        self, model_name=\"xgboost_v1.pkl\", preprocessor_name=\"preprocessor_v1.pkl\"\n    ):\n        \"\"\"\n        Loads the trained model (XGBoost) and preprocessor (Scaler/Encoder).\n        \"\"\"\n        self.model_path = MODELS_PATH / model_name\n        self.preprocessor_path = MODELS_PATH / preprocessor_name\n\n        try:\n            self.model = joblib.load(self.model_path)\n            self.preprocessor = joblib.load(self.preprocessor_path)\n            logger.info(f\"Model and Preprocessor loaded from {MODELS_PATH}\")\n        except Exception as e:\n            logger.error(f\"Critical error loading model: {e}\")\n            self.model = None\n            self.preprocessor = None\n\n    def predict_batch(self, df_input: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Predicts intensity for a batch of stations (DataFrame).\n\n        Args:\n            df_input: DataFrame containing all raw features (cols from DB + Lags).\n\n        Returns:\n            pd.DataFrame: Input DataFrame enriched with 'predicted_intensity' column.\n        \"\"\"\n        if not self.model or not self.preprocessor:\n            logger.error(\"Model not loaded. Cannot predict.\")\n            return None\n\n        try:\n            # 1. Transform features (Scaling/Encoding) using the pre-fitted preprocessor\n            X_processed, _ = self.preprocessor.transform(df_input)\n\n            # 2. Predict\n            predictions = self.model.predict(X_processed)\n\n            # 3. Format results\n            results = np.maximum(0, np.round(predictions)).astype(int)\n\n            # Add prediction to DataFrame\n            df_result = df_input.copy()\n            df_result[\"predicted_intensity\"] = results\n\n            return df_result\n\n        except Exception as e:\n            logger.error(f\"Error during batch prediction: {e}\")\n            return None\n</code></pre> <p>handler: python options: members: - predict_daily_batch - save_predictions_to_db show_root_heading: true show_source: true</p>"},{"location":"ml/modeling/#modeling.predictor.TrafficPredictor.__init__","title":"<code>__init__(model_name='xgboost_v1.pkl', preprocessor_name='preprocessor_v1.pkl')</code>","text":"<p>Loads the trained model (XGBoost) and preprocessor (Scaler/Encoder).</p> Source code in <code>backend/modeling/predictor.py</code> <pre><code>def __init__(\n    self, model_name=\"xgboost_v1.pkl\", preprocessor_name=\"preprocessor_v1.pkl\"\n):\n    \"\"\"\n    Loads the trained model (XGBoost) and preprocessor (Scaler/Encoder).\n    \"\"\"\n    self.model_path = MODELS_PATH / model_name\n    self.preprocessor_path = MODELS_PATH / preprocessor_name\n\n    try:\n        self.model = joblib.load(self.model_path)\n        self.preprocessor = joblib.load(self.preprocessor_path)\n        logger.info(f\"Model and Preprocessor loaded from {MODELS_PATH}\")\n    except Exception as e:\n        logger.error(f\"Critical error loading model: {e}\")\n        self.model = None\n        self.preprocessor = None\n</code></pre>"},{"location":"ml/modeling/#modeling.predictor.TrafficPredictor.predict_batch","title":"<code>predict_batch(df_input)</code>","text":"<p>Predicts intensity for a batch of stations (DataFrame).</p> <p>Parameters:</p> Name Type Description Default <code>df_input</code> <code>DataFrame</code> <p>DataFrame containing all raw features (cols from DB + Lags).</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Input DataFrame enriched with 'predicted_intensity' column.</p> Source code in <code>backend/modeling/predictor.py</code> <pre><code>def predict_batch(self, df_input: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Predicts intensity for a batch of stations (DataFrame).\n\n    Args:\n        df_input: DataFrame containing all raw features (cols from DB + Lags).\n\n    Returns:\n        pd.DataFrame: Input DataFrame enriched with 'predicted_intensity' column.\n    \"\"\"\n    if not self.model or not self.preprocessor:\n        logger.error(\"Model not loaded. Cannot predict.\")\n        return None\n\n    try:\n        # 1. Transform features (Scaling/Encoding) using the pre-fitted preprocessor\n        X_processed, _ = self.preprocessor.transform(df_input)\n\n        # 2. Predict\n        predictions = self.model.predict(X_processed)\n\n        # 3. Format results\n        results = np.maximum(0, np.round(predictions)).astype(int)\n\n        # Add prediction to DataFrame\n        df_result = df_input.copy()\n        df_result[\"predicted_intensity\"] = results\n\n        return df_result\n\n    except Exception as e:\n        logger.error(f\"Error during batch prediction: {e}\")\n        return None\n</code></pre>"},{"location":"ml/modeling/#lentraineur-training","title":"L'Entra\u00eeneur (Training)","text":"<p>Classe responsable de la GridSearch et de la validation crois\u00e9e.</p> Source code in <code>backend/modeling/trainer.py</code> <pre><code>class ModelTrainer:\n    def __init__(self):\n        self.model = xgb.XGBRegressor(\n            objective='reg:squarederror',\n            n_estimators=1000,\n            n_jobs=-1\n        )\n        self.best_model = None\n\n    def train(self, X, y):\n        \"\"\"Entra\u00eenement avec validation crois\u00e9e temporelle.\"\"\"\n        logger.info(\"--- D\u00e9marrage de l'entra\u00eenement XGBoost ---\")\n\n        # D\u00e9finition des hyperparam\u00e8tres \u00e0 tester pour le GridSearchCV\n        param_grid = {\n            'max_depth': [3, 5, 7],\n            'learning_rate': [0.01, 0.1],\n            'n_estimators': [100, 500, 1000]\n        }\n        # Validation crois\u00e9e temporelle (=/= train_test_split, non adapt\u00e9 aux s\u00e9ries temporelles)\n        tscv = TimeSeriesSplit(n_splits=3)\n        # Recherche des hyperparam\u00e8tres via GridSearchCV\n        grid_search = GridSearchCV(\n            estimator=self.model,\n            param_grid=param_grid,\n            cv=tscv,\n            scoring='neg_root_mean_squared_error',\n            verbose=0,\n            n_jobs=-1\n        )\n\n        logger.info(\"Recherche des meilleurs hyperparam\u00e8tres (GridSearch)...\")\n        grid_search.fit(X, y)\n\n        self.best_model = grid_search.best_estimator_\n\n        logger.info(f\"Meilleurs param\u00e8tres trouv\u00e9s : {grid_search.best_params_}\")\n        logger.info(f\"Meilleur score (RMSE) : {-grid_search.best_score_:.2f}\")\n\n    def evaluate(self, X_test, y_test):\n        \"\"\"\u00c9valuation sur le jeu de test.\"\"\"\n        if not self.best_model:\n            logger.error(\"Tentative d'\u00e9valuation sans mod\u00e8le entra\u00een\u00e9.\")\n            return\n\n        predictions = self.best_model.predict(X_test)\n\n        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n        mae = mean_absolute_error(y_test, predictions)\n\n        logger.info(\"--- R\u00e9sultats de l'\u00e9valuation ---\")\n        logger.info(f\"RMSE (Erreur quadratique moyenne) : {rmse:.2f}\")\n        logger.info(f\"MAE (Erreur absolue moyenne)    : {mae:.2f}\")\n\n        return rmse\n\n    def save(self, filename=\"xgboost_model.pkl\"):\n            \"\"\"\n            Sauvegarde le mod\u00e8le entra\u00een\u00e9.\n            Si 'filename' est un nom simple, utilise MODELS_PATH.\n            \"\"\"\n            if self.best_model:\n                try:\n                    path_obj = Path(filename)\n\n                    # Si le chemin n'a pas de dossier parent\n                    if len(path_obj.parts) == 1:\n                        target_path = MODELS_PATH / filename\n                    else:\n                        target_path = path_obj\n\n                    # Cr\u00e9ation du dossier parent si besoin\n                    target_path.parent.mkdir(parents=True, exist_ok=True)\n\n                    joblib.dump(self.best_model, target_path)\n                    logger.info(f\"Mod\u00e8le sauvegard\u00e9 sous : {target_path}\")\n                except Exception as e:\n                    logger.error(f\"Erreur lors de la sauvegarde du mod\u00e8le : {e}\")\n            else:\n                logger.warning(\"Aucun mod\u00e8le \u00e0 sauvegarder (entra\u00eenement non effectu\u00e9 ou \u00e9chou\u00e9).\")\n</code></pre> <p>handler: python options: show_root_heading: true</p>"},{"location":"ml/modeling/#modeling.trainer.ModelTrainer.evaluate","title":"<code>evaluate(X_test, y_test)</code>","text":"<p>\u00c9valuation sur le jeu de test.</p> Source code in <code>backend/modeling/trainer.py</code> <pre><code>def evaluate(self, X_test, y_test):\n    \"\"\"\u00c9valuation sur le jeu de test.\"\"\"\n    if not self.best_model:\n        logger.error(\"Tentative d'\u00e9valuation sans mod\u00e8le entra\u00een\u00e9.\")\n        return\n\n    predictions = self.best_model.predict(X_test)\n\n    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n    mae = mean_absolute_error(y_test, predictions)\n\n    logger.info(\"--- R\u00e9sultats de l'\u00e9valuation ---\")\n    logger.info(f\"RMSE (Erreur quadratique moyenne) : {rmse:.2f}\")\n    logger.info(f\"MAE (Erreur absolue moyenne)    : {mae:.2f}\")\n\n    return rmse\n</code></pre>"},{"location":"ml/modeling/#modeling.trainer.ModelTrainer.save","title":"<code>save(filename='xgboost_model.pkl')</code>","text":"<p>Sauvegarde le mod\u00e8le entra\u00een\u00e9. Si 'filename' est un nom simple, utilise MODELS_PATH.</p> Source code in <code>backend/modeling/trainer.py</code> <pre><code>def save(self, filename=\"xgboost_model.pkl\"):\n        \"\"\"\n        Sauvegarde le mod\u00e8le entra\u00een\u00e9.\n        Si 'filename' est un nom simple, utilise MODELS_PATH.\n        \"\"\"\n        if self.best_model:\n            try:\n                path_obj = Path(filename)\n\n                # Si le chemin n'a pas de dossier parent\n                if len(path_obj.parts) == 1:\n                    target_path = MODELS_PATH / filename\n                else:\n                    target_path = path_obj\n\n                # Cr\u00e9ation du dossier parent si besoin\n                target_path.parent.mkdir(parents=True, exist_ok=True)\n\n                joblib.dump(self.best_model, target_path)\n                logger.info(f\"Mod\u00e8le sauvegard\u00e9 sous : {target_path}\")\n            except Exception as e:\n                logger.error(f\"Erreur lors de la sauvegarde du mod\u00e8le : {e}\")\n        else:\n            logger.warning(\"Aucun mod\u00e8le \u00e0 sauvegarder (entra\u00eenement non effectu\u00e9 ou \u00e9chou\u00e9).\")\n</code></pre>"},{"location":"ml/modeling/#modeling.trainer.ModelTrainer.train","title":"<code>train(X, y)</code>","text":"<p>Entra\u00eenement avec validation crois\u00e9e temporelle.</p> Source code in <code>backend/modeling/trainer.py</code> <pre><code>def train(self, X, y):\n    \"\"\"Entra\u00eenement avec validation crois\u00e9e temporelle.\"\"\"\n    logger.info(\"--- D\u00e9marrage de l'entra\u00eenement XGBoost ---\")\n\n    # D\u00e9finition des hyperparam\u00e8tres \u00e0 tester pour le GridSearchCV\n    param_grid = {\n        'max_depth': [3, 5, 7],\n        'learning_rate': [0.01, 0.1],\n        'n_estimators': [100, 500, 1000]\n    }\n    # Validation crois\u00e9e temporelle (=/= train_test_split, non adapt\u00e9 aux s\u00e9ries temporelles)\n    tscv = TimeSeriesSplit(n_splits=3)\n    # Recherche des hyperparam\u00e8tres via GridSearchCV\n    grid_search = GridSearchCV(\n        estimator=self.model,\n        param_grid=param_grid,\n        cv=tscv,\n        scoring='neg_root_mean_squared_error',\n        verbose=0,\n        n_jobs=-1\n    )\n\n    logger.info(\"Recherche des meilleurs hyperparam\u00e8tres (GridSearch)...\")\n    grid_search.fit(X, y)\n\n    self.best_model = grid_search.best_estimator_\n\n    logger.info(f\"Meilleurs param\u00e8tres trouv\u00e9s : {grid_search.best_params_}\")\n    logger.info(f\"Meilleur score (RMSE) : {-grid_search.best_score_:.2f}\")\n</code></pre>"},{"location":"ml/monitoring/","title":"Monitoring de Performance","text":"<p>Le monitoring est essentiel pour d\u00e9tecter une d\u00e9rive du mod\u00e8le (Data Drift ou Model Drift).</p>"},{"location":"ml/monitoring/#methodologie","title":"M\u00e9thodologie","text":"<ol> <li> <p>Le calcul de performance est d\u00e9clench\u00e9 \u00e0 J+1, une fois que les donn\u00e9es r\u00e9elles sont disponibles.</p> </li> <li> <p>Le syst\u00e8me r\u00e9cup\u00e8re les pr\u00e9dictions faites \u00e0 la date D.</p> </li> <li> <p>Le syst\u00e8me r\u00e9cup\u00e8re les donn\u00e9es r\u00e9elles de la date D.</p> </li> <li> <p>Une jointure (Inner Join) est faite sur l'ID de station.</p> </li> <li> <p>On calcule l'Erreur Absolue (|Predit - Reel|).</p> </li> </ol>"},{"location":"ml/monitoring/#implementation","title":"Impl\u00e9mentation","text":"Source code in <code>backend/monitoring/performance.py</code> <pre><code>class PerformanceMonitor:\n    def __init__(self, session: Session):\n        self.session = session\n        self.service = DatabaseService(session)\n\n    def run_daily_evaluation(self, evaluation_date: datetime):\n        \"\"\"\n        Compare les pr\u00e9dictions vs la r\u00e9alit\u00e9 pour une date donn\u00e9e (J-1).\n        Calcule les erreurs et remplit la table model_metrics.\n        \"\"\"\n        date_str = evaluation_date.strftime('%Y-%m-%d')\n        logger.info(f\"D\u00e9marrage du Monitoring pour la date : {date_str}\")\n\n        # 1. R\u00e9cup\u00e9ration des donn\u00e9es (Pr\u00e9dictions vs R\u00e9alit\u00e9)\n        preds_list = self.service.get_predictions_by_date(evaluation_date)\n        actuals_list = self.service.get_actuals_by_date(evaluation_date)\n\n        # S\u00e9curit\u00e9s de base\n        if not preds_list:\n            logger.warning(f\"Aucune pr\u00e9diction trouv\u00e9e pour le {date_str}. Impossible d'\u00e9valuer.\")\n            return\n\n        if not actuals_list:\n            logger.warning(f\"Aucune donn\u00e9e r\u00e9elle trouv\u00e9e pour le {date_str}. Le pipeline de mise \u00e0 jour a-t-il tourn\u00e9 ?\")\n            return\n\n        # 2. Conversion en DataFrames \n        df_preds = pd.DataFrame([{\n            \"station_id\": p.station_id, \n            \"predicted_value\": p.prediction_value,\n            \"model_version\": p.model_version\n        } for p in preds_list])\n\n        df_actuals = pd.DataFrame([{\n            \"station_id\": a.station_id, \n            \"actual_value\": a.intensity\n        } for a in actuals_list])\n\n        # 3. Fusion (Inner Join) : On ne note que ce qui est comparable\n        # Cela \u00e9limine les stations qui ont une pr\u00e9diction mais pas de donn\u00e9e r\u00e9elle (panne) et inversement\n        df_metrics = pd.merge(df_preds, df_actuals, on=\"station_id\", how=\"inner\")\n\n        if df_metrics.empty:\n            logger.error(\"Erreur : Aucune station commune trouv\u00e9e entre les pr\u00e9dictions et la r\u00e9alit\u00e9.\")\n            return\n\n        # 4. Calcul des Erreurs Math\u00e9matiques\n        # Erreur Absolue ( |Pred - R\u00e9el| )\n        df_metrics['absolute_error'] = (df_metrics['predicted_value'] - df_metrics['actual_value']).abs()\n\n        # MAE du Jour (Moyenne des erreurs absolues)\n        # C'est l'indicateur cl\u00e9 : \"En moyenne aujourd'hui, on s'est tromp\u00e9 de X v\u00e9los\"\n        daily_mae = df_metrics['absolute_error'].mean()\n\n        logger.info(f\"Performance du jour ({len(df_metrics)} stations) : MAE = {daily_mae:.2f}\")\n\n        # 5. Pr\u00e9paration pour la sauvegarde en BDD\n        # On construit la liste de dictionnaires pour le bulk_insert\n        metrics_to_save = []\n        for _, row in df_metrics.iterrows():\n            metrics_to_save.append({\n                \"date\": evaluation_date,\n                \"station_id\": row['station_id'],\n                \"actual_value\": int(row['actual_value']),\n                \"predicted_value\": int(row['predicted_value']),\n                \"absolute_error\": float(row['absolute_error']),\n                \"mean_absolute_error\": float(daily_mae), # On r\u00e9p\u00e8te la m\u00e9trique globale sur chaque ligne pour faciliter les requ\u00eates SQL futures\n                \"model_version\": row['model_version']\n            })\n\n        # 6. Appel au service pour ins\u00e9rer\n        try:\n            # Utilisation de la m\u00e9thode existante add_model_metrics\n            success = self.service.add_model_metrics(metrics_to_save)\n            if success:\n                self.session.commit()\n                logger.info(\"M\u00e9triques sauvegard\u00e9es avec succ\u00e8s dans 'model_metrics'.\")\n            else:\n                logger.error(\"Echec de l'insertion des m\u00e9triques.\")\n        except Exception as e:\n            self.session.rollback()\n            logger.error(f\"Erreur critique lors du commit des m\u00e9triques : {e}\")\n</code></pre> <p>handler: python options: members: - run_daily_evaluation show_root_heading: true</p>"},{"location":"ml/monitoring/#monitoring.performance.PerformanceMonitor.run_daily_evaluation","title":"<code>run_daily_evaluation(evaluation_date)</code>","text":"<p>Compare les pr\u00e9dictions vs la r\u00e9alit\u00e9 pour une date donn\u00e9e (J-1). Calcule les erreurs et remplit la table model_metrics.</p> Source code in <code>backend/monitoring/performance.py</code> <pre><code>def run_daily_evaluation(self, evaluation_date: datetime):\n    \"\"\"\n    Compare les pr\u00e9dictions vs la r\u00e9alit\u00e9 pour une date donn\u00e9e (J-1).\n    Calcule les erreurs et remplit la table model_metrics.\n    \"\"\"\n    date_str = evaluation_date.strftime('%Y-%m-%d')\n    logger.info(f\"D\u00e9marrage du Monitoring pour la date : {date_str}\")\n\n    # 1. R\u00e9cup\u00e9ration des donn\u00e9es (Pr\u00e9dictions vs R\u00e9alit\u00e9)\n    preds_list = self.service.get_predictions_by_date(evaluation_date)\n    actuals_list = self.service.get_actuals_by_date(evaluation_date)\n\n    # S\u00e9curit\u00e9s de base\n    if not preds_list:\n        logger.warning(f\"Aucune pr\u00e9diction trouv\u00e9e pour le {date_str}. Impossible d'\u00e9valuer.\")\n        return\n\n    if not actuals_list:\n        logger.warning(f\"Aucune donn\u00e9e r\u00e9elle trouv\u00e9e pour le {date_str}. Le pipeline de mise \u00e0 jour a-t-il tourn\u00e9 ?\")\n        return\n\n    # 2. Conversion en DataFrames \n    df_preds = pd.DataFrame([{\n        \"station_id\": p.station_id, \n        \"predicted_value\": p.prediction_value,\n        \"model_version\": p.model_version\n    } for p in preds_list])\n\n    df_actuals = pd.DataFrame([{\n        \"station_id\": a.station_id, \n        \"actual_value\": a.intensity\n    } for a in actuals_list])\n\n    # 3. Fusion (Inner Join) : On ne note que ce qui est comparable\n    # Cela \u00e9limine les stations qui ont une pr\u00e9diction mais pas de donn\u00e9e r\u00e9elle (panne) et inversement\n    df_metrics = pd.merge(df_preds, df_actuals, on=\"station_id\", how=\"inner\")\n\n    if df_metrics.empty:\n        logger.error(\"Erreur : Aucune station commune trouv\u00e9e entre les pr\u00e9dictions et la r\u00e9alit\u00e9.\")\n        return\n\n    # 4. Calcul des Erreurs Math\u00e9matiques\n    # Erreur Absolue ( |Pred - R\u00e9el| )\n    df_metrics['absolute_error'] = (df_metrics['predicted_value'] - df_metrics['actual_value']).abs()\n\n    # MAE du Jour (Moyenne des erreurs absolues)\n    # C'est l'indicateur cl\u00e9 : \"En moyenne aujourd'hui, on s'est tromp\u00e9 de X v\u00e9los\"\n    daily_mae = df_metrics['absolute_error'].mean()\n\n    logger.info(f\"Performance du jour ({len(df_metrics)} stations) : MAE = {daily_mae:.2f}\")\n\n    # 5. Pr\u00e9paration pour la sauvegarde en BDD\n    # On construit la liste de dictionnaires pour le bulk_insert\n    metrics_to_save = []\n    for _, row in df_metrics.iterrows():\n        metrics_to_save.append({\n            \"date\": evaluation_date,\n            \"station_id\": row['station_id'],\n            \"actual_value\": int(row['actual_value']),\n            \"predicted_value\": int(row['predicted_value']),\n            \"absolute_error\": float(row['absolute_error']),\n            \"mean_absolute_error\": float(daily_mae), # On r\u00e9p\u00e8te la m\u00e9trique globale sur chaque ligne pour faciliter les requ\u00eates SQL futures\n            \"model_version\": row['model_version']\n        })\n\n    # 6. Appel au service pour ins\u00e9rer\n    try:\n        # Utilisation de la m\u00e9thode existante add_model_metrics\n        success = self.service.add_model_metrics(metrics_to_save)\n        if success:\n            self.session.commit()\n            logger.info(\"M\u00e9triques sauvegard\u00e9es avec succ\u00e8s dans 'model_metrics'.\")\n        else:\n            logger.error(\"Echec de l'insertion des m\u00e9triques.\")\n    except Exception as e:\n        self.session.rollback()\n        logger.error(f\"Erreur critique lors du commit des m\u00e9triques : {e}\")\n</code></pre>"},{"location":"ml/monitoring/#stockage","title":"Stockage","text":"<p>Les m\u00e9triques sont stock\u00e9es dans la table model_metrics. Cela permet de requ\u00eater facilement le MAE (Mean Absolute Error) moyen par jour pour visualiser la sant\u00e9 du mod\u00e8le sur un dashboard.</p>"},{"location":"pipelines/extract/","title":"Collecte de Donn\u00e9es (Extract)","text":"<p>Le module download contient les connecteurs vers les APIs externes.</p>"},{"location":"pipelines/extract/#strategie-de-pagination-chunking","title":"Strat\u00e9gie de Pagination (Chunking)","text":"<p>L'API EcoCompteur de Montpellier impose une limite de taille par requ\u00eate. Pour contourner cela, nous avons impl\u00e9ment\u00e9 une strat\u00e9gie de d\u00e9coupage temporel :</p> <ul> <li> <p>La p\u00e9riode totale est divis\u00e9e en sous-p\u00e9riodes (chunks) de 6 mois.</p> </li> <li> <p>Si la p\u00e9riode demand\u00e9e est d'un seul jour (mise \u00e0 jour quotidienne), un chunk unique est g\u00e9n\u00e9r\u00e9.</p> </li> <li> <p>Les r\u00e9sultats sont agr\u00e9g\u00e9s dans une liste unique avant d'\u00eatre retourn\u00e9s.</p> </li> </ul>"},{"location":"pipelines/extract/#api-trafic","title":"API Trafic","text":"<p>               Bases: <code>BaseAPILoader</code></p> <p>Loader to fetch time series data from Montpellier Ecocounter API for multiple stations. Handles API pagination limits (max 10k records) by chunking the time range.</p> <p>Inherits from BaseAPILoader and implements the abstract method <code>fetch_data</code>.</p> Source code in <code>backend/download/trafic_history_api.py</code> <pre><code>class EcoCounterTimeseriesLoader(BaseAPILoader):\n    \"\"\"\n    Loader to fetch time series data from Montpellier Ecocounter API for multiple stations.\n    Handles API pagination limits (max 10k records) by chunking the time range.\n\n    Inherits from BaseAPILoader and implements the abstract method `fetch_data`.\n    \"\"\"\n\n    def _generate_date_chunks(self, start_str: str, end_str: str, months: int = 6) -&gt; List[Tuple[str, str]]:\n        \"\"\"\n        Helper method to split a date range into smaller chunks (e.g., 6 months).\n        \"\"\"\n        start = pd.to_datetime(start_str)\n        end = pd.to_datetime(end_str)\n        chunks = []\n        # If the range is a single day (or instantaneous), return one chunk immediately\n        if start == end:\n            return [(\n                start.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n                end.strftime(\"%Y-%m-%dT%H:%M:%S\")\n            )]\n\n        current = start\n        while current &lt; end:\n            next_date = current + pd.DateOffset(months=months)\n            chunk_end = min(next_date, end)\n\n            # Format expected by the API (YYYY-MM-DDTHH:MM:SS)\n            chunks.append((\n                current.strftime(\"%Y-%m-%dT%H:%M:%S\"),\n                chunk_end.strftime(\"%Y-%m-%dT%H:%M:%S\")\n            ))\n            # Next chunk starts where the previous one ended\n            current = chunk_end\n\n        return chunks\n\n    def fetch_data(\n        self,\n        station_ids_list: List[str],\n        start_date: str,\n        end_date: str,\n        timeout: int = 5,\n        retries: int = 3,\n    ) -&gt; Dict[str, Optional[Dict]]:\n        \"\"\"\n        Loop over station IDs AND date chunks to fetch complete JSON data with retry mechanism.\n        \"\"\"\n        results: Dict[str, Optional[Dict]] = {}\n        total_stations = len(station_ids_list)\n\n        # 1. Prepare date chunks to bypass API limits\n        date_chunks = self._generate_date_chunks(start_date, end_date)\n        logger.info(f\"Time range split into {len(date_chunks)} chunks per station to ensure full data retrieval.\")\n\n        for idx, station_id in enumerate(station_ids_list, start=1):\n            full_id = (\n                f\"urn:ngsi-ld:EcoCounter:{station_id}\"\n                if \"urn:\" not in station_id\n                else station_id\n            )\n            encoded_id = urllib.parse.quote(full_id)\n            url = f\"https://portail-api-data.montpellier3m.fr/ecocounter_timeseries/{encoded_id}/attrs/intensity\"\n\n            print(\n                f\"[{idx}/{total_stations}] Downloading data for station {station_id}...\",\n                end=\" \",\n                flush=True,\n            )\n\n            # Containers to merge results from different time chunks\n            consolidated_index = []\n            consolidated_values = []\n\n            # 2. Loop over time chunks\n            for chunk_start, chunk_end in date_chunks:\n                params = {\"fromDate\": chunk_start, \"toDate\": chunk_end}\n                chunk_success = False\n\n                for attempt in range(retries):\n                    try:\n                        response = requests.get(url, params=params, timeout=timeout)\n\n                        # Specific handling: 404 on a chunk isn't fatal (just no data for this period)\n                        if response.status_code == 404:\n                            chunk_success = True\n                            break\n\n                        response.raise_for_status()\n                        data = response.json()\n\n                        # Merge data into the main lists\n                        if \"index\" in data and \"values\" in data:\n                            consolidated_index.extend(data[\"index\"])\n                            consolidated_values.extend(data[\"values\"])\n\n                        chunk_success = True\n                        # Success for this chunk, exit retry loop\n                        break \n\n                    except Timeout:\n                        logger.warning(\n                            f\"Timeout on attempt {attempt + 1} for station {station_id} (chunk {chunk_start})\"\n                        )\n\n                    except ConnectionError:\n                        logger.warning(\n                            f\"Connection error on attempt {attempt + 1} for station {station_id}\"\n                        )\n\n                    except HTTPError as http_err:\n                        # Log critical errors (other than handled 404)\n                        logger.error(f\"HTTP error {http_err} for station {station_id}\")\n                        break\n\n                    except ValueError as json_err:\n                        logger.error(\n                            f\"JSON decode error {json_err} for station {station_id}\"\n                        )\n                        break\n\n                    except RequestException as req_err:\n                        logger.error(\n                            f\"Request exception {req_err} for station {station_id}\"\n                        )\n\n                    time.sleep(0.5 * (attempt + 1))\n\n                # End of retry loop for this chunk\n\n            # 3. Reconstruct the final JSON response structure for this station\n            if consolidated_index:\n                results[station_id] = {\n                    \"index\": consolidated_index,\n                    \"values\": consolidated_values,\n                    \"id\": station_id\n                }\n                print(\"Done (Merged)\")\n            else:\n                results[station_id] = None\n                print(\"No Data or Failed\")\n\n        return results\n</code></pre> <p>handler: python options: members: - fetch_data - _generate_date_chunks show_root_heading: true</p>"},{"location":"pipelines/extract/#download.trafic_history_api.EcoCounterTimeseriesLoader.fetch_data","title":"<code>fetch_data(station_ids_list, start_date, end_date, timeout=5, retries=3)</code>","text":"<p>Loop over station IDs AND date chunks to fetch complete JSON data with retry mechanism.</p> Source code in <code>backend/download/trafic_history_api.py</code> <pre><code>def fetch_data(\n    self,\n    station_ids_list: List[str],\n    start_date: str,\n    end_date: str,\n    timeout: int = 5,\n    retries: int = 3,\n) -&gt; Dict[str, Optional[Dict]]:\n    \"\"\"\n    Loop over station IDs AND date chunks to fetch complete JSON data with retry mechanism.\n    \"\"\"\n    results: Dict[str, Optional[Dict]] = {}\n    total_stations = len(station_ids_list)\n\n    # 1. Prepare date chunks to bypass API limits\n    date_chunks = self._generate_date_chunks(start_date, end_date)\n    logger.info(f\"Time range split into {len(date_chunks)} chunks per station to ensure full data retrieval.\")\n\n    for idx, station_id in enumerate(station_ids_list, start=1):\n        full_id = (\n            f\"urn:ngsi-ld:EcoCounter:{station_id}\"\n            if \"urn:\" not in station_id\n            else station_id\n        )\n        encoded_id = urllib.parse.quote(full_id)\n        url = f\"https://portail-api-data.montpellier3m.fr/ecocounter_timeseries/{encoded_id}/attrs/intensity\"\n\n        print(\n            f\"[{idx}/{total_stations}] Downloading data for station {station_id}...\",\n            end=\" \",\n            flush=True,\n        )\n\n        # Containers to merge results from different time chunks\n        consolidated_index = []\n        consolidated_values = []\n\n        # 2. Loop over time chunks\n        for chunk_start, chunk_end in date_chunks:\n            params = {\"fromDate\": chunk_start, \"toDate\": chunk_end}\n            chunk_success = False\n\n            for attempt in range(retries):\n                try:\n                    response = requests.get(url, params=params, timeout=timeout)\n\n                    # Specific handling: 404 on a chunk isn't fatal (just no data for this period)\n                    if response.status_code == 404:\n                        chunk_success = True\n                        break\n\n                    response.raise_for_status()\n                    data = response.json()\n\n                    # Merge data into the main lists\n                    if \"index\" in data and \"values\" in data:\n                        consolidated_index.extend(data[\"index\"])\n                        consolidated_values.extend(data[\"values\"])\n\n                    chunk_success = True\n                    # Success for this chunk, exit retry loop\n                    break \n\n                except Timeout:\n                    logger.warning(\n                        f\"Timeout on attempt {attempt + 1} for station {station_id} (chunk {chunk_start})\"\n                    )\n\n                except ConnectionError:\n                    logger.warning(\n                        f\"Connection error on attempt {attempt + 1} for station {station_id}\"\n                    )\n\n                except HTTPError as http_err:\n                    # Log critical errors (other than handled 404)\n                    logger.error(f\"HTTP error {http_err} for station {station_id}\")\n                    break\n\n                except ValueError as json_err:\n                    logger.error(\n                        f\"JSON decode error {json_err} for station {station_id}\"\n                    )\n                    break\n\n                except RequestException as req_err:\n                    logger.error(\n                        f\"Request exception {req_err} for station {station_id}\"\n                    )\n\n                time.sleep(0.5 * (attempt + 1))\n\n            # End of retry loop for this chunk\n\n        # 3. Reconstruct the final JSON response structure for this station\n        if consolidated_index:\n            results[station_id] = {\n                \"index\": consolidated_index,\n                \"values\": consolidated_values,\n                \"id\": station_id\n            }\n            print(\"Done (Merged)\")\n        else:\n            results[station_id] = None\n            print(\"No Data or Failed\")\n\n    return results\n</code></pre>"},{"location":"pipelines/extract/#api-meteo","title":"API M\u00e9t\u00e9o","text":"<p>Nous utilisons OpenMeteo pour l'historique et les pr\u00e9visions.</p> <p>A client class to fetch daily weather data from Open-Meteo API. Uses caching and automatic retry on errors.</p> Source code in <code>backend/download/daily_weather_api.py</code> <pre><code>class OpenMeteoDailyAPIC:\n    \"\"\"\n    A client class to fetch daily weather data from Open-Meteo API.\n    Uses caching and automatic retry on errors.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the API client with cache and retry configuration.\n        \"\"\"\n        try:\n            cache_session = requests_cache.CachedSession(\n                CACHE_PATH / \".cache_daily\", expire_after=3600\n            )\n            self.session = retry(cache_session, retries=5, backoff_factor=0.2)\n            self.client = openmeteo_requests.Client(session=self.session)\n        except Exception as e:\n            logger.error(f\"Failed to initialize Open-Meteo client: {e}\")\n            raise\n\n    def get_weather_json(\n        self,\n        latitude: float,\n        longitude: float,\n        start_date: str,\n        end_date: str,\n        daily_variables: List[str] = [\n            \"temperature_2m_mean\",\n            \"wind_speed_10m_mean\",\n            \"precipitation_sum\",\n        ],\n        timezone: str = \"Europe/London\",\n    ) -&gt; Any:\n        \"\"\"\n        Fetch daily weather data from Open-Meteo API for the given coordinates and date range.\n\n        Args:\n            latitude (float): Latitude of the location.\n            longitude (float): Longitude of the location.\n            start_date (str): Start date in YYYY-MM-DD format.\n            end_date (str): End date in YYYY-MM-DD format.\n            daily_variables (List[str], optional): List of daily variables to fetch.\n                Defaults to temperature, wind speed, and precipitation.\n            timezone (str, optional): Timezone for the response. Defaults to \"Europe/London\".\n\n        Returns:\n            Any: JSON-like response object from Open-Meteo API.\n\n        Raises:\n            ValueError: If the API returns no data.\n            requests.exceptions.RequestException: If a network or HTTP error occurs.\n            Exception: For other unexpected errors.\n        \"\"\"\n        try:\n            url = \"https://api.open-meteo.com/v1/forecast\"\n            params = {\n                \"latitude\": latitude,\n                \"longitude\": longitude,\n                \"daily\": daily_variables,\n                \"timezone\": timezone,\n                \"start_date\": start_date,\n                \"end_date\": end_date,\n            }\n\n            responses = self.client.weather_api(url, params=params)\n\n            if not responses or len(responses) == 0:\n                raise ValueError(\"No data returned from Open-Meteo API.\")\n\n            return responses\n\n        except requests.exceptions.RequestException as req_err:\n            logger.error(f\"Network or HTTP error: {req_err}\")\n            raise\n\n        except IndexError as idx_err:\n            logger.error(\n                f\"Unexpected response structure from Open-Meteo API: {idx_err}\"\n            )\n            raise\n\n        except ValueError as val_err:\n            logger.error(f\"Data error: {val_err}\")\n            raise\n\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred: {e}\")\n            raise\n</code></pre> <p>handler: python options: members: - get_weather_json show_root_heading: true</p>"},{"location":"pipelines/extract/#download.daily_weather_api.OpenMeteoDailyAPIC.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the API client with cache and retry configuration.</p> Source code in <code>backend/download/daily_weather_api.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the API client with cache and retry configuration.\n    \"\"\"\n    try:\n        cache_session = requests_cache.CachedSession(\n            CACHE_PATH / \".cache_daily\", expire_after=3600\n        )\n        self.session = retry(cache_session, retries=5, backoff_factor=0.2)\n        self.client = openmeteo_requests.Client(session=self.session)\n    except Exception as e:\n        logger.error(f\"Failed to initialize Open-Meteo client: {e}\")\n        raise\n</code></pre>"},{"location":"pipelines/extract/#download.daily_weather_api.OpenMeteoDailyAPIC.get_weather_json","title":"<code>get_weather_json(latitude, longitude, start_date, end_date, daily_variables=['temperature_2m_mean', 'wind_speed_10m_mean', 'precipitation_sum'], timezone='Europe/London')</code>","text":"<p>Fetch daily weather data from Open-Meteo API for the given coordinates and date range.</p> <p>Parameters:</p> Name Type Description Default <code>latitude</code> <code>float</code> <p>Latitude of the location.</p> required <code>longitude</code> <code>float</code> <p>Longitude of the location.</p> required <code>start_date</code> <code>str</code> <p>Start date in YYYY-MM-DD format.</p> required <code>end_date</code> <code>str</code> <p>End date in YYYY-MM-DD format.</p> required <code>daily_variables</code> <code>List[str]</code> <p>List of daily variables to fetch. Defaults to temperature, wind speed, and precipitation.</p> <code>['temperature_2m_mean', 'wind_speed_10m_mean', 'precipitation_sum']</code> <code>timezone</code> <code>str</code> <p>Timezone for the response. Defaults to \"Europe/London\".</p> <code>'Europe/London'</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>JSON-like response object from Open-Meteo API.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the API returns no data.</p> <code>RequestException</code> <p>If a network or HTTP error occurs.</p> <code>Exception</code> <p>For other unexpected errors.</p> Source code in <code>backend/download/daily_weather_api.py</code> <pre><code>def get_weather_json(\n    self,\n    latitude: float,\n    longitude: float,\n    start_date: str,\n    end_date: str,\n    daily_variables: List[str] = [\n        \"temperature_2m_mean\",\n        \"wind_speed_10m_mean\",\n        \"precipitation_sum\",\n    ],\n    timezone: str = \"Europe/London\",\n) -&gt; Any:\n    \"\"\"\n    Fetch daily weather data from Open-Meteo API for the given coordinates and date range.\n\n    Args:\n        latitude (float): Latitude of the location.\n        longitude (float): Longitude of the location.\n        start_date (str): Start date in YYYY-MM-DD format.\n        end_date (str): End date in YYYY-MM-DD format.\n        daily_variables (List[str], optional): List of daily variables to fetch.\n            Defaults to temperature, wind speed, and precipitation.\n        timezone (str, optional): Timezone for the response. Defaults to \"Europe/London\".\n\n    Returns:\n        Any: JSON-like response object from Open-Meteo API.\n\n    Raises:\n        ValueError: If the API returns no data.\n        requests.exceptions.RequestException: If a network or HTTP error occurs.\n        Exception: For other unexpected errors.\n    \"\"\"\n    try:\n        url = \"https://api.open-meteo.com/v1/forecast\"\n        params = {\n            \"latitude\": latitude,\n            \"longitude\": longitude,\n            \"daily\": daily_variables,\n            \"timezone\": timezone,\n            \"start_date\": start_date,\n            \"end_date\": end_date,\n        }\n\n        responses = self.client.weather_api(url, params=params)\n\n        if not responses or len(responses) == 0:\n            raise ValueError(\"No data returned from Open-Meteo API.\")\n\n        return responses\n\n    except requests.exceptions.RequestException as req_err:\n        logger.error(f\"Network or HTTP error: {req_err}\")\n        raise\n\n    except IndexError as idx_err:\n        logger.error(\n            f\"Unexpected response structure from Open-Meteo API: {idx_err}\"\n        )\n        raise\n\n    except ValueError as val_err:\n        logger.error(f\"Data error: {val_err}\")\n        raise\n\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred: {e}\")\n        raise\n</code></pre>"},{"location":"pipelines/load/","title":"Gestion de la Base de Donn\u00e9es (Load)","text":"<p>Toutes les interactions SQL sont centralis\u00e9es dans le DatabaseService.</p>"},{"location":"pipelines/load/#modele-de-donnees","title":"Mod\u00e8le de Donn\u00e9es","text":"<p>L'application utilise 4 tables principales :</p> <p>counters_info : R\u00e9f\u00e9rentiel des stations.</p> <p>bike_count : Historique r\u00e9el (Trafic).</p> <p>weather : Historique m\u00e9t\u00e9o.</p> <p>predictions : R\u00e9sultats du mod\u00e8le.</p>"},{"location":"pipelines/load/#strategies-dinsertion","title":"Strat\u00e9gies d'Insertion","text":"<p>Nous utilisons deux approches selon le contexte :</p> <p>Insertion de Masse (Bulk) : Utilis\u00e9e pour l'historique. Rapide, mais ne retourne pas les IDs g\u00e9n\u00e9r\u00e9s.</p> <p>Insertion Transactionnelle (Single) : Utilis\u00e9e pour les pr\u00e9dictions quotidiennes. Plus lente, mais permet de r\u00e9cup\u00e9rer l'ID de la pr\u00e9diction pour y lier le contexte (Features JSON).</p> <p>Service layer for database operations. This class abstracts the database session and provides methods to interact with the data models.</p> Source code in <code>backend/database/service.py</code> <pre><code>class DatabaseService:\n    \"\"\"\n    Service layer for database operations.\n    This class abstracts the database session and provides\n    methods to interact with the data models.\n    \"\"\"\n\n    def __init__(self, session: Session):\n        self.session = session\n\n    def _bulk_add(self, model: Type[Base], data: List[Dict[str, Any]]) -&gt; bool:\n        \"\"\"\n        Generic method to bulk insert a list of dictionaries into a table.\n        \"\"\"\n        if not data:\n            logger.info(f\"No data provided for model {model.__tablename__}. Skipping.\")\n            return True\n        try:\n            self.session.bulk_insert_mappings(model, data)\n            logger.info(f\"Prepared {len(data)} records for {model.__tablename__}.\")\n            return True\n        except SQLAlchemyError as e:\n            logger.error(f\"Error during bulk insert for {model.__tablename__}: {e}\")\n            return False\n\n    def add_counter_infos(self, counters_data: List[Dict[str, Any]]) -&gt; bool:\n        \"\"\"\n        Add counters if they do not already exist (based on station_id).\n        \"\"\"\n        if not counters_data:\n            logger.info(\"No counter data provided. Skipping.\")\n            return True\n\n        # Retrieve all existing IDs in the database\n        existing_ids = {\n            res[0] for res in self.session.query(CounterInfo.station_id).all()\n        }\n\n        # Filter the incoming list\n        new_counters = []\n        seen_ids_in_batch = set()\n\n        for counter in counters_data:\n            s_id = counter.get(\"station_id\")\n\n            # If the ID already exists in the database -&gt; Ignore\n            if s_id in existing_ids:\n                continue\n\n            # If it has already been seen in the current file -&gt; Ignore (CSV duplicate)\n            if s_id in seen_ids_in_batch:\n                continue\n\n            # It's a real new counter.\n            seen_ids_in_batch.add(s_id)\n            new_counters.append(counter)\n\n        if not new_counters:\n            logger.info(\"All provided counters already exist.\")\n            return True\n\n        # Insertion\n        try:\n            self.session.bulk_insert_mappings(CounterInfo, new_counters)\n            logger.info(f\"Successfully inserted {len(new_counters)} new counters.\")\n            return True\n        except SQLAlchemyError as e:\n            logger.error(f\"Error bulk inserting counters: {e}\")\n            return False\n\n    def add_bike_counts(self, counts_data: List[Dict[str, Any]]):\n        \"\"\"Adds multiple bike count records to the database.\"\"\"\n        return self._bulk_add(BikeCount, counts_data)\n\n    def add_weather_data(self, weather_data: List[Dict[str, Any]]):\n        \"\"\"Adds multiple weather records to the database.\"\"\"\n        return self._bulk_add(Weather, weather_data)\n\n    def add_predictions(self, predictions_data: List[Dict[str, Any]]):\n        \"\"\"Add multiple predictions records to the database\"\"\"\n        return self._bulk_add(Prediction, predictions_data)\n\n    def add_model_metrics(self, model_metrics: List[Dict[str, Any]]):\n        \"\"\"Add multiples model metrics records to the database\"\"\"\n        return self._bulk_add(ModelMetrics, model_metrics)\n\n    def get_latest_prediction_for_counter(\n        self, station_id: str\n    ) -&gt; Optional[Prediction]:\n        \"\"\"\n        Retrieves the most recent prediction for a given counter.\n        \"\"\"\n        try:\n            result = (\n                self.session.query(Prediction)\n                .filter(Prediction.station_id == station_id)\n                .order_by(Prediction.prediction_date.desc())\n                .first()\n            )\n            return result\n        except SQLAlchemyError as e:\n            logger.error(f\"Error fetching prediction for counter {station_id}: {e}\")\n            return None\n\n    # --- Logique de r\u00e9cup\u00e9ration des donn\u00e9es ---\n\n    def get_all_stations(self) -&gt; List[CounterInfo]:\n        \"\"\"Retrieves all active stations from the database.\"\"\"\n        return self.session.query(CounterInfo).all()\n\n    def get_weather_for_date(self, target_date: datetime) -&gt; Optional[Weather]:\n        \"\"\"Retrieves weather forecast/data for a specific date.\"\"\"\n        # Assuming date is stored at midnight or date-only format in DB\n        return self.session.query(Weather).filter(Weather.date == target_date).first()\n\n    def get_bike_count(self, station_id: str, target_date: datetime) -&gt; Optional[int]:\n        \"\"\"\n        Retrieves the real intensity for a specific station and date.\n        Used to reconstruct Lags (J-1, J-7) for prediction.\n        \"\"\"\n        result = (\n            self.session.query(BikeCount.intensity)\n            .filter(BikeCount.station_id == station_id)\n            .filter(BikeCount.date == target_date)\n            .first()\n        )\n        return result[0] if result else None\n\n    def get_most_recent_bike_count(self, station_id: str) -&gt; Optional[BikeCount]:\n        \"\"\"Retrieves the most recent bike count record for a station.\"\"\"\n        try:\n            return (\n                self.session.query(BikeCount)\n                .filter(BikeCount.station_id == station_id)\n                .order_by(BikeCount.date.desc())\n                .first()\n            )\n        except SQLAlchemyError as e:\n            logger.error(f\"Error fetching most recent count for {station_id}: {e}\")\n            return None\n\n    def save_prediction_single_with_context(\n        self, pred_data: Dict, features_data: Dict\n    ) -&gt; bool:\n        \"\"\"\n        Saves ONE prediction AND its associated JSON context in a single transaction.\n        Uses .flush() to retrieve the generated prediction ID before creating the context entry.\n        \"\"\"\n        try:\n            # 1. Create Prediction Object\n            prediction = Prediction(**pred_data)\n            self.session.add(prediction)\n\n            # 2. Flush: Send to DB to generate prediction.id (transaction remains open)\n            self.session.flush()\n\n            # 3. Create linked FeaturesData Object\n            features = FeaturesData(\n                prediction_id=prediction.id,  # The Foreign Key is now available\n                station_id=pred_data[\"station_id\"],\n                date=pred_data[\"prediction_date\"],\n                features=features_data,  # SQLAlchemy automatically handles Dict -&gt; JSON conversion\n                target_intensity=None,\n            )\n            self.session.add(features)\n\n            # 4. Final Commit (Both rows saved together)\n            self.session.commit()\n            return True\n\n        except SQLAlchemyError as e:\n            self.session.rollback()\n            logger.error(\n                f\"Transaction failed for prediction {pred_data.get('station_id')}: {e}\"\n            )\n            return False\n\n    # --- Monitoring ---#\n\n    def get_predictions_by_date(self, target_date: datetime) -&gt; List[Prediction]:\n        \"\"\"\n        Retrieves all predictions made for a target date.\n        \"\"\"\n        # Filter by date\n        return (\n            self.session.query(Prediction)\n            .filter(Prediction.prediction_date == target_date)\n            .all()\n        )\n\n    def get_actuals_by_date(self, target_date: datetime) -&gt; List[BikeCount]:\n        \"\"\"\n        Retrieves actual counts for a given date.\n        Useful for monitoring (the reality on the ground).\n        \"\"\"\n        return self.session.query(BikeCount).filter(BikeCount.date == target_date).all()\n\n    def get_dashboard_stats(self, station_id: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Retrieves and aggregates all statistics for the frontend dashboard.\n        \"\"\"\n        today = datetime.now().date()\n\n        # 30-day history (BikeCount)\n        start_30d = today - timedelta(days=30)\n        rows_30d = (\n            self.session.query(BikeCount.date, BikeCount.intensity)\n            .filter(BikeCount.station_id == station_id)\n            .filter(BikeCount.date &gt;= start_30d)\n            .order_by(BikeCount.date.asc())\n            .all()\n        )\n        # We just extract the intensities.\n        # Note: If days are missing, the graph will be short; ideally, the gaps should be filled in.\n        history_30_days = [r.intensity for r in rows_30d]\n\n        # 7-day accuracy (Actual vs. Forecast comparison)\n        start_7d = today - timedelta(days=7)\n\n        # Past predictions\n        preds_7d = (\n            self.session.query(Prediction.prediction_date, Prediction.prediction_value)\n            .filter(Prediction.station_id == station_id)\n            .filter(Prediction.prediction_date &gt;= start_7d)\n            .filter(Prediction.prediction_date &lt; today)  # Strictly before today\n            .all()\n        )\n        # Real past\n        reals_7d = (\n            self.session.query(BikeCount.date, BikeCount.intensity)\n            .filter(BikeCount.station_id == station_id)\n            .filter(BikeCount.date &gt;= start_7d)\n            .filter(BikeCount.date &lt; today)\n            .all()\n        )\n\n        # Alignment via dictionary (Date -&gt; Value)\n        # We use .date() to ensure that we are comparing days, not hours.\n        dict_pred = {p.prediction_date.date(): p.prediction_value for p in preds_7d}\n        dict_real = {r.date.date(): r.intensity for r in reals_7d}\n\n        dates_last_7 = [start_7d + timedelta(days=i) for i in range(7)]\n        acc_real = [dict_real.get(d, 0) for d in dates_last_7]\n        acc_pred = [dict_pred.get(d, 0) for d in dates_last_7]\n\n        # Averages per day of the week (Weekly Profile)\n        # We take a broad historical view (90 days)\n        start_90d = today - timedelta(days=90)\n        rows_90d = (\n            self.session.query(BikeCount.date, BikeCount.intensity)\n            .filter(BikeCount.station_id == station_id)\n            .filter(BikeCount.date &gt;= start_90d)\n            .all()\n        )\n\n        week_sums = [0] * 7\n        week_counts = [0] * 7\n\n        for r in rows_90d:\n            wd = r.date.weekday()  # 0=Monday, 6=Sunday\n            week_sums[wd] += r.intensity\n            week_counts[wd] += 1\n\n        weekly_averages = [\n            int(s / c) if c &gt; 0 else 0 for s, c in zip(week_sums, week_counts)\n        ]\n\n        # Weekly Totals (12-week volume)\n        # For now, if there is insufficient historical data, an empty safe list is returned.\n        weekly_totals = [0] * 12\n\n        return {\n            \"history_30_days\": history_30_days,\n            \"accuracy_7_days\": {\"real\": acc_real, \"pred\": acc_pred},\n            \"weekly_averages\": weekly_averages,\n            \"weekly_totals\": weekly_totals,\n        }\n</code></pre> <p>handler: python options: members: - add_bike_counts - save_prediction_single_with_context - get_bike_count show_root_heading: true</p>"},{"location":"pipelines/load/#database.service.DatabaseService.add_bike_counts","title":"<code>add_bike_counts(counts_data)</code>","text":"<p>Adds multiple bike count records to the database.</p> Source code in <code>backend/database/service.py</code> <pre><code>def add_bike_counts(self, counts_data: List[Dict[str, Any]]):\n    \"\"\"Adds multiple bike count records to the database.\"\"\"\n    return self._bulk_add(BikeCount, counts_data)\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.add_counter_infos","title":"<code>add_counter_infos(counters_data)</code>","text":"<p>Add counters if they do not already exist (based on station_id).</p> Source code in <code>backend/database/service.py</code> <pre><code>def add_counter_infos(self, counters_data: List[Dict[str, Any]]) -&gt; bool:\n    \"\"\"\n    Add counters if they do not already exist (based on station_id).\n    \"\"\"\n    if not counters_data:\n        logger.info(\"No counter data provided. Skipping.\")\n        return True\n\n    # Retrieve all existing IDs in the database\n    existing_ids = {\n        res[0] for res in self.session.query(CounterInfo.station_id).all()\n    }\n\n    # Filter the incoming list\n    new_counters = []\n    seen_ids_in_batch = set()\n\n    for counter in counters_data:\n        s_id = counter.get(\"station_id\")\n\n        # If the ID already exists in the database -&gt; Ignore\n        if s_id in existing_ids:\n            continue\n\n        # If it has already been seen in the current file -&gt; Ignore (CSV duplicate)\n        if s_id in seen_ids_in_batch:\n            continue\n\n        # It's a real new counter.\n        seen_ids_in_batch.add(s_id)\n        new_counters.append(counter)\n\n    if not new_counters:\n        logger.info(\"All provided counters already exist.\")\n        return True\n\n    # Insertion\n    try:\n        self.session.bulk_insert_mappings(CounterInfo, new_counters)\n        logger.info(f\"Successfully inserted {len(new_counters)} new counters.\")\n        return True\n    except SQLAlchemyError as e:\n        logger.error(f\"Error bulk inserting counters: {e}\")\n        return False\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.add_model_metrics","title":"<code>add_model_metrics(model_metrics)</code>","text":"<p>Add multiples model metrics records to the database</p> Source code in <code>backend/database/service.py</code> <pre><code>def add_model_metrics(self, model_metrics: List[Dict[str, Any]]):\n    \"\"\"Add multiples model metrics records to the database\"\"\"\n    return self._bulk_add(ModelMetrics, model_metrics)\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.add_predictions","title":"<code>add_predictions(predictions_data)</code>","text":"<p>Add multiple predictions records to the database</p> Source code in <code>backend/database/service.py</code> <pre><code>def add_predictions(self, predictions_data: List[Dict[str, Any]]):\n    \"\"\"Add multiple predictions records to the database\"\"\"\n    return self._bulk_add(Prediction, predictions_data)\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.add_weather_data","title":"<code>add_weather_data(weather_data)</code>","text":"<p>Adds multiple weather records to the database.</p> Source code in <code>backend/database/service.py</code> <pre><code>def add_weather_data(self, weather_data: List[Dict[str, Any]]):\n    \"\"\"Adds multiple weather records to the database.\"\"\"\n    return self._bulk_add(Weather, weather_data)\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_actuals_by_date","title":"<code>get_actuals_by_date(target_date)</code>","text":"<p>Retrieves actual counts for a given date. Useful for monitoring (the reality on the ground).</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_actuals_by_date(self, target_date: datetime) -&gt; List[BikeCount]:\n    \"\"\"\n    Retrieves actual counts for a given date.\n    Useful for monitoring (the reality on the ground).\n    \"\"\"\n    return self.session.query(BikeCount).filter(BikeCount.date == target_date).all()\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_all_stations","title":"<code>get_all_stations()</code>","text":"<p>Retrieves all active stations from the database.</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_all_stations(self) -&gt; List[CounterInfo]:\n    \"\"\"Retrieves all active stations from the database.\"\"\"\n    return self.session.query(CounterInfo).all()\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_bike_count","title":"<code>get_bike_count(station_id, target_date)</code>","text":"<p>Retrieves the real intensity for a specific station and date. Used to reconstruct Lags (J-1, J-7) for prediction.</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_bike_count(self, station_id: str, target_date: datetime) -&gt; Optional[int]:\n    \"\"\"\n    Retrieves the real intensity for a specific station and date.\n    Used to reconstruct Lags (J-1, J-7) for prediction.\n    \"\"\"\n    result = (\n        self.session.query(BikeCount.intensity)\n        .filter(BikeCount.station_id == station_id)\n        .filter(BikeCount.date == target_date)\n        .first()\n    )\n    return result[0] if result else None\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_dashboard_stats","title":"<code>get_dashboard_stats(station_id)</code>","text":"<p>Retrieves and aggregates all statistics for the frontend dashboard.</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_dashboard_stats(self, station_id: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Retrieves and aggregates all statistics for the frontend dashboard.\n    \"\"\"\n    today = datetime.now().date()\n\n    # 30-day history (BikeCount)\n    start_30d = today - timedelta(days=30)\n    rows_30d = (\n        self.session.query(BikeCount.date, BikeCount.intensity)\n        .filter(BikeCount.station_id == station_id)\n        .filter(BikeCount.date &gt;= start_30d)\n        .order_by(BikeCount.date.asc())\n        .all()\n    )\n    # We just extract the intensities.\n    # Note: If days are missing, the graph will be short; ideally, the gaps should be filled in.\n    history_30_days = [r.intensity for r in rows_30d]\n\n    # 7-day accuracy (Actual vs. Forecast comparison)\n    start_7d = today - timedelta(days=7)\n\n    # Past predictions\n    preds_7d = (\n        self.session.query(Prediction.prediction_date, Prediction.prediction_value)\n        .filter(Prediction.station_id == station_id)\n        .filter(Prediction.prediction_date &gt;= start_7d)\n        .filter(Prediction.prediction_date &lt; today)  # Strictly before today\n        .all()\n    )\n    # Real past\n    reals_7d = (\n        self.session.query(BikeCount.date, BikeCount.intensity)\n        .filter(BikeCount.station_id == station_id)\n        .filter(BikeCount.date &gt;= start_7d)\n        .filter(BikeCount.date &lt; today)\n        .all()\n    )\n\n    # Alignment via dictionary (Date -&gt; Value)\n    # We use .date() to ensure that we are comparing days, not hours.\n    dict_pred = {p.prediction_date.date(): p.prediction_value for p in preds_7d}\n    dict_real = {r.date.date(): r.intensity for r in reals_7d}\n\n    dates_last_7 = [start_7d + timedelta(days=i) for i in range(7)]\n    acc_real = [dict_real.get(d, 0) for d in dates_last_7]\n    acc_pred = [dict_pred.get(d, 0) for d in dates_last_7]\n\n    # Averages per day of the week (Weekly Profile)\n    # We take a broad historical view (90 days)\n    start_90d = today - timedelta(days=90)\n    rows_90d = (\n        self.session.query(BikeCount.date, BikeCount.intensity)\n        .filter(BikeCount.station_id == station_id)\n        .filter(BikeCount.date &gt;= start_90d)\n        .all()\n    )\n\n    week_sums = [0] * 7\n    week_counts = [0] * 7\n\n    for r in rows_90d:\n        wd = r.date.weekday()  # 0=Monday, 6=Sunday\n        week_sums[wd] += r.intensity\n        week_counts[wd] += 1\n\n    weekly_averages = [\n        int(s / c) if c &gt; 0 else 0 for s, c in zip(week_sums, week_counts)\n    ]\n\n    # Weekly Totals (12-week volume)\n    # For now, if there is insufficient historical data, an empty safe list is returned.\n    weekly_totals = [0] * 12\n\n    return {\n        \"history_30_days\": history_30_days,\n        \"accuracy_7_days\": {\"real\": acc_real, \"pred\": acc_pred},\n        \"weekly_averages\": weekly_averages,\n        \"weekly_totals\": weekly_totals,\n    }\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_latest_prediction_for_counter","title":"<code>get_latest_prediction_for_counter(station_id)</code>","text":"<p>Retrieves the most recent prediction for a given counter.</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_latest_prediction_for_counter(\n    self, station_id: str\n) -&gt; Optional[Prediction]:\n    \"\"\"\n    Retrieves the most recent prediction for a given counter.\n    \"\"\"\n    try:\n        result = (\n            self.session.query(Prediction)\n            .filter(Prediction.station_id == station_id)\n            .order_by(Prediction.prediction_date.desc())\n            .first()\n        )\n        return result\n    except SQLAlchemyError as e:\n        logger.error(f\"Error fetching prediction for counter {station_id}: {e}\")\n        return None\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_most_recent_bike_count","title":"<code>get_most_recent_bike_count(station_id)</code>","text":"<p>Retrieves the most recent bike count record for a station.</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_most_recent_bike_count(self, station_id: str) -&gt; Optional[BikeCount]:\n    \"\"\"Retrieves the most recent bike count record for a station.\"\"\"\n    try:\n        return (\n            self.session.query(BikeCount)\n            .filter(BikeCount.station_id == station_id)\n            .order_by(BikeCount.date.desc())\n            .first()\n        )\n    except SQLAlchemyError as e:\n        logger.error(f\"Error fetching most recent count for {station_id}: {e}\")\n        return None\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_predictions_by_date","title":"<code>get_predictions_by_date(target_date)</code>","text":"<p>Retrieves all predictions made for a target date.</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_predictions_by_date(self, target_date: datetime) -&gt; List[Prediction]:\n    \"\"\"\n    Retrieves all predictions made for a target date.\n    \"\"\"\n    # Filter by date\n    return (\n        self.session.query(Prediction)\n        .filter(Prediction.prediction_date == target_date)\n        .all()\n    )\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.get_weather_for_date","title":"<code>get_weather_for_date(target_date)</code>","text":"<p>Retrieves weather forecast/data for a specific date.</p> Source code in <code>backend/database/service.py</code> <pre><code>def get_weather_for_date(self, target_date: datetime) -&gt; Optional[Weather]:\n    \"\"\"Retrieves weather forecast/data for a specific date.\"\"\"\n    # Assuming date is stored at midnight or date-only format in DB\n    return self.session.query(Weather).filter(Weather.date == target_date).first()\n</code></pre>"},{"location":"pipelines/load/#database.service.DatabaseService.save_prediction_single_with_context","title":"<code>save_prediction_single_with_context(pred_data, features_data)</code>","text":"<p>Saves ONE prediction AND its associated JSON context in a single transaction. Uses .flush() to retrieve the generated prediction ID before creating the context entry.</p> Source code in <code>backend/database/service.py</code> <pre><code>def save_prediction_single_with_context(\n    self, pred_data: Dict, features_data: Dict\n) -&gt; bool:\n    \"\"\"\n    Saves ONE prediction AND its associated JSON context in a single transaction.\n    Uses .flush() to retrieve the generated prediction ID before creating the context entry.\n    \"\"\"\n    try:\n        # 1. Create Prediction Object\n        prediction = Prediction(**pred_data)\n        self.session.add(prediction)\n\n        # 2. Flush: Send to DB to generate prediction.id (transaction remains open)\n        self.session.flush()\n\n        # 3. Create linked FeaturesData Object\n        features = FeaturesData(\n            prediction_id=prediction.id,  # The Foreign Key is now available\n            station_id=pred_data[\"station_id\"],\n            date=pred_data[\"prediction_date\"],\n            features=features_data,  # SQLAlchemy automatically handles Dict -&gt; JSON conversion\n            target_intensity=None,\n        )\n        self.session.add(features)\n\n        # 4. Final Commit (Both rows saved together)\n        self.session.commit()\n        return True\n\n    except SQLAlchemyError as e:\n        self.session.rollback()\n        logger.error(\n            f\"Transaction failed for prediction {pred_data.get('station_id')}: {e}\"\n        )\n        return False\n</code></pre>"},{"location":"pipelines/overview/","title":"Orchestration et Pipelines","text":"<p>Cette section d\u00e9crit les diff\u00e9rents processus d'ex\u00e9cution du projet. Nous distinguons les processus d'initialisation (One-shot) des processus r\u00e9currents (Batchs quotidiens).</p>"},{"location":"pipelines/overview/#les-points-dentree","title":"Les Points d'Entr\u00e9e","text":"<p>L'application dispose de deux points d'entr\u00e9e principaux situ\u00e9s \u00e0 la racine du module backend :</p>"},{"location":"pipelines/overview/#main_initializepy-script-dinstallation","title":"main_initialize.py : Script d'installation.","text":"<ul> <li> <p>Initialise la base de donn\u00e9es.</p> </li> <li> <p>R\u00e9cup\u00e8re tout l'historique (2023-Aujourd'hui).</p> </li> <li> <p>Entra\u00eene le premier mod\u00e8le.</p> </li> </ul>"},{"location":"pipelines/overview/#mainpy-interface-cli-menu-pour-le-debogage-et-lexecution-manuelle-des-taches-quotidiennes","title":"main.py : Interface CLI (Menu) pour le d\u00e9bogage et l'ex\u00e9cution manuelle des t\u00e2ches quotidiennes.","text":"<p>Le Flux Quotidien (Daily Batch)</p> <p>En production, le syst\u00e8me ex\u00e9cute deux scripts s\u00e9quentiellement chaque matin :</p> <pre><code>\ngraph TD\n    A[\"D\u00e9clencheur (07:00)\"] --&gt; B(\"daily_update.py\")\n    B --&gt; C{\"Donn\u00e9es J-1 dispo ?\"}\n    C -- Oui --&gt; D[\"Mise \u00e0 jour BDD\"]\n    D --&gt; E[\"Monitoring Performance\"]\n    C -- Non --&gt; F[\"Log Warning &amp; Arr\u00eat\"]\n    E --&gt; G(\"daily_prediction.py\")\n    G --&gt; H[\"R\u00e9cup\u00e9ration M\u00e9t\u00e9o J0\"]\n    H --&gt; I[\"Construction Lags via BDD\"]\n    I --&gt; J[\"Inf\u00e9rence &amp; Sauvegarde\"]\n</code></pre> <p>Mise \u00e0 Jour (daily_update) : R\u00e9cup\u00e8re la v\u00e9rit\u00e9 terrain de la veille.</p> <p>Pr\u00e9diction (daily_prediction) : Estime le trafic du jour en utilisant les donn\u00e9es fra\u00eechement ins\u00e9r\u00e9es.</p>"}]}